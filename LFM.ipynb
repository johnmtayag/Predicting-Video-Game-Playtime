{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4de33b3-b307-469a-b4b3-68de6838659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528eb567-f457-418b-9c2b-9830f82ae52b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### EVALUATION / METRICS\n",
    "######################################\n",
    "def convert_to_np_array(A):\n",
    "    \"\"\"\n",
    "    If A is not already an array, convert it to an array\n",
    "    \"\"\"\n",
    "    if not isinstance(A, np.ndarray): return np.array(A)\n",
    "    else: return A\n",
    "\n",
    "def get_MSE(A, B):\n",
    "    \"\"\"\n",
    "    Given list A and list B:\n",
    "    Return the mean squared error between A and B\n",
    "    \"\"\"\n",
    "    return np.mean((convert_to_np_array(A) - convert_to_np_array(B))**2)\n",
    "\n",
    "def inner(A, B):\n",
    "    \"\"\"\n",
    "    Return the dot product between list A and list B\n",
    "    \"\"\"\n",
    "    return np.dot(convert_to_np_array(A), convert_to_np_array(B))\n",
    "\n",
    "def get_SSE(A, B):\n",
    "    \"\"\"\n",
    "    Given list A and list B:\n",
    "    Return the sum of squared errors between A and B\n",
    "    \"\"\"\n",
    "    return np.sum((convert_to_np_array(A) - convert_to_np_array(B))**2)\n",
    "\n",
    "def get_SE(A,B):\n",
    "    \"\"\"\n",
    "    Given list A and list B:\n",
    "    Return the squared error between each element\n",
    "    \"\"\"\n",
    "    return (convert_to_np_array(A) - convert_to_np_array(B))**2\n",
    "\n",
    "def get_accuracy(A,B):\n",
    "    \"\"\"\n",
    "    Given list A and list B:\n",
    "    Return the accuracy\n",
    "    \"\"\"\n",
    "    return np.sum(convert_to_np_array(A) == convert_to_np_array(B)) / len(A)\n",
    "\n",
    "def get_BER(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    \"Return the balanced error rate between positive (1) and negative(0) instances\n",
    "    \"\"\"\n",
    "\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    n_pos, n_neg = 0, 0\n",
    "    for actual, pred in zip(y_actual, y_predicted):\n",
    "        if actual==1:\n",
    "            n_pos += 1\n",
    "            if actual==pred:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            n_neg += 1\n",
    "            if actual==pred:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (FN + TP)\n",
    "        \n",
    "    return (1/2) * (FPR + FNR)\n",
    "\n",
    "def get_errorMetrics_binary(y_actual, y_predicted, beta=1):\n",
    "    \"\"\"\n",
    "    Return a set of error metrics between positive (1) and negative (0) instances\n",
    "    This is valid for a binary class case\n",
    "    Return a dictionary containing all calculated values\n",
    "    \"\"\"\n",
    "\n",
    "    output = {}\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    n_pos, n_neg = 0, 0\n",
    "    for actual, pred in zip(y_actual, y_predicted):\n",
    "        if actual==1:\n",
    "            n_pos += 1\n",
    "            if actual==pred:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            n_neg += 1\n",
    "            if actual==pred:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    ###\n",
    "    TPR, FNR = TP / n_pos, FN / n_pos\n",
    "    FPR, TNR = FP / n_neg, TN / n_neg\n",
    "    prec = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    output[\"TP\"], output[\"FP\"], output[\"TN\"], output[\"FN\"] = TP, FP, TN, FN\n",
    "    output[\"TPR\"], output[\"FPR\"], output[\"FNR\"], output[\"TNR\"] = TPR, FPR, FNR, TNR\n",
    "    output[\"precision\"], output[\"recall\"] = prec, recall\n",
    "    output[\"BER\"] = (1/2) * (FPR + FNR)\n",
    "    output[f\"F{beta}_Score\"] = (1 + beta**2) * (prec * recall) / ((beta**2)*prec + recall)\n",
    "    output[\"F_Score\"] = 2 * (prec * recall) / (prec + recall)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffaeb450-6a88-4200-9501-3a52ce2902ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### SIMILARITY FUNCTIONS\n",
    "######################################\n",
    "def jaccard_sim(A,B):\n",
    "    \"\"\"\n",
    "    Return the Jaccard similarity between list A and list B\n",
    "    \"\"\"\n",
    "    if not isinstance(A, set): A = set(A)\n",
    "    if not isinstance(B, set): B = set(B)\n",
    "    n_intersect = len(A.intersection(B))\n",
    "    if n_intersect == 0: return 0\n",
    "    n_union = len(A.union(B))\n",
    "    if n_union == 0: return 0\n",
    "\n",
    "    return n_intersect / n_union\n",
    "\n",
    "def cosine_sim_binary(A,B, denom_over_all=True):\n",
    "    \"\"\"\n",
    "    Return the cosine similarity between set A and set B (Binary interactions)\n",
    "    \"\"\"\n",
    "    if not isinstance(A, set): A = set(A)\n",
    "    if not isinstance(B, set): B = set(B)\n",
    "    n_intersect = len(A.intersection(B))\n",
    "    # if n_intersect == 0: return 0\n",
    "\n",
    "    if denom_over_all:\n",
    "        total_interactions = np.sqrt(len(A) * len(B))\n",
    "    else:\n",
    "        total_interactions = n_intersect\n",
    "    if total_interactions == 0:\n",
    "        return 0\n",
    "    return n_intersect / total_interactions\n",
    "\n",
    "############# Design structures to record shared items\n",
    "def cosine_sim(x_tuple, y_tuple, denom_over_all):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between lists x and y\n",
    "    Input are lists of tuples: [(id1, rating), (id2, rating), ...]\n",
    "    \"\"\"\n",
    "    # Get shared items\n",
    "    x_ids, y_ids = set(), set()\n",
    "    x_ratings, y_ratings = [], []\n",
    "    shared_ratings_x, shared_ratings_y = [], []\n",
    "    shared_tuples_x, shared_tuples_y = [], []\n",
    "    for tuple in x_tuple:\n",
    "        x_ids.add(tuple[0])\n",
    "        x_ratings.append(tuple[1])\n",
    "    for tuple in y_tuple:\n",
    "        y_ids.add(tuple[0])\n",
    "        y_ratings.append(tuple[1])\n",
    "    shared_ids = x_ids.intersection(y_ids)\n",
    "    # if len(shared_ids) == 0: return 0\n",
    "\n",
    "    shared_tuples_x = [tuple for tuple in x_tuple if tuple[0] in shared_ids]\n",
    "    shared_tuples_x.sort()\n",
    "    shared_tuples_y = [tuple for tuple in y_tuple if tuple[0] in shared_ids]\n",
    "    shared_tuples_y.sort()\n",
    "    shared_ratings_x = [tuple[1] for tuple in shared_tuples_x]\n",
    "    shared_ratings_y = [tuple[1] for tuple in shared_tuples_y]\n",
    "\n",
    "    if denom_over_all:\n",
    "        # Use all items in the denominator\n",
    "        x_norm = np.sum([xi**2 for xi in x_ratings])\n",
    "        y_norm = np.sum([yi**2 for yi in y_ratings])\n",
    "    else:\n",
    "        # Only use shared items in the denominator\n",
    "        x_norm = np.sum([xi**2 for xi in shared_ratings_x])\n",
    "        y_norm = np.sum([yi**2 for yi in shared_ratings_y])\n",
    "    denom = np.sqrt(x_norm) * np.sqrt(y_norm)\n",
    "\n",
    "    if denom == 0: return 0\n",
    "    numer = sum([xi*yi for xi,yi in zip(shared_ratings_x, shared_ratings_y)])\n",
    "\n",
    "    return numer / denom\n",
    "\n",
    "def pearson_sim(x_tuple, y_tuple):\n",
    "    \"\"\"\n",
    "    Calculate the pearson similarity between lists x and y\n",
    "    Input are lists of tuples: [(id1, rating), (id2, rating), ...]\n",
    "    Unlike Cosine sim, ONLY shared items can be considered\n",
    "    If id1 or id2 is not in the relevant training data structure, use meanValue as its respective mean\n",
    "    \"\"\"\n",
    "    # Unpack averages\n",
    "    x_avgs = {tuple[0][0]:tuple[1] for tuple in x_tuple}\n",
    "    y_avgs = {tuple[0][0]:tuple[1] for tuple in y_tuple}\n",
    "    # Get shared items\n",
    "    shared_ratings_x, shared_ratings_y = [], []\n",
    "    shared_tuples_x, shared_tuples_y = [], []\n",
    "    x_ids = {tuple[0][0] for tuple in x_tuple}\n",
    "    y_ids = {tuple[0][0] for tuple in y_tuple}\n",
    "    shared_ids = x_ids.intersection(y_ids)\n",
    "    # if len(shared_ids) == 0: return 0\n",
    "\n",
    "    shared_tuples_x = [tuple[0] for tuple in x_tuple if tuple[0][0] in shared_ids]\n",
    "    shared_tuples_x.sort()\n",
    "    shared_tuples_y = [tuple[0] for tuple in y_tuple if tuple[0][0] in shared_ids]\n",
    "    shared_tuples_y.sort()\n",
    "    shared_ratings_x = [tuple[1] - x_avgs[tuple[0]] for tuple in shared_tuples_x] ### Pearson --> Subtract the mean from each value\n",
    "    shared_ratings_y = [tuple[1] - y_avgs[tuple[0]] for tuple in shared_tuples_y]\n",
    "\n",
    "    # Only use shared items in the denominator\n",
    "    x_norm = np.sum([xi**2 for xi in shared_ratings_x])\n",
    "    y_norm = np.sum([yi**2 for yi in shared_ratings_y])\n",
    "    denom = np.sqrt(x_norm * y_norm)\n",
    "\n",
    "    if denom == 0: return 0\n",
    "    numer = sum([xi*yi for xi,yi in zip(shared_ratings_x, shared_ratings_y)])\n",
    "\n",
    "    return numer / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963b42d1-a6ab-46e6-82a5-f6dd1827f5b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### COLLABORATIVE FILTERING\n",
    "######################################\n",
    "\n",
    "\n",
    "def predictValue_bySim_devFromMean(user_id, item_id, sim_func, type, meanValue, value_bounds=None, denom_over_all=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Predict some value (e.g., rating) that the user (user_id) will give an item (item_id) based on\n",
    "    the input simularity function (sim_func). However, instead of predicting the rating directly,\n",
    "    predict the deviation from the global mean rating\n",
    "\n",
    "    # Necessary Data Structures --> These are built using training data ONLY\n",
    "    itemsPerUser: A dictionary containing the list of items each user interacted with and corresponding values\n",
    "       ex: itemsPerUser[user1] = [(item1, 2), (item3, 1), (item5, 5), ...]\n",
    "    usersPerItem: A dictionary containing the list of users that interacted with each item and corresponding values\n",
    "       ex: usersPerItem[item1] = [(user1, 2), (user4, 2), (user27, 2), ...]\n",
    "    itemAverages: A dictionary containing the mean value for each item\n",
    "    userAverages: A dictionary containing the mean value for each user\n",
    "    #--- value_bounds: The min/max values that can be outputted as typically there's a scale (e.g., 1-5 stars)\n",
    "    #--- meanValue: The mean of all values in valueDict.values()\n",
    "\n",
    "    # Gathering similarity weights:\n",
    "    If type==0:\n",
    "       # Predict the rating as a weighted sum of ratings that user_id has given to other items #\n",
    "       For each item (item_id2) that user_id has interacted with (except for item_id):\n",
    "          Calculate item_id2's similarity to item_id based on shared user interactions\n",
    "          Track these values as similarity weights\n",
    "    else:\n",
    "       # Predict the rating as a weighted sum of ratings that other users have given to item_id #\n",
    "       For each user (user_id2) that have interacted with item_id (except for user_id):\n",
    "          Calculate user_id2's similarity to user_id based on shared item interactions\n",
    "          Track these values as similarity weights\n",
    "    \"\"\"\n",
    "    # Edge case 1: Return global mean value if user_id or item_id are unseen\n",
    "    if (user_id not in itemsPerUser) or (item_id not in usersPerItem): return meanValue\n",
    "    # Initialize variables\n",
    "    if denom_over_all is None: denom_over_all = True\n",
    "    if value_bounds is None: value_bounds = (-np.inf, np.inf)\n",
    "    values, similarities = [], []\n",
    "\n",
    "    if type == \"item\":\n",
    "        # Predict the rating as a weighted combination of how other items rated by user_id were\n",
    "        # rated by similar users\n",
    "        # if user_id not in userAverages: return meanValue # Skip keys that are not in the dict\n",
    "        avg_value = userAverages[user_id]\n",
    "        for user_id2,value in usersPerItem[item_id]:\n",
    "            if user_id2 == user_id: continue\n",
    "            # if user_id2 not in userAverages: continue  # Skip keys that are not in the dict\n",
    "            if sim_func == jaccard_sim:\n",
    "                simset_user_id = {tuple[0] for tuple in itemsPerUser[user_id] if tuple[0] != item_id}\n",
    "                simset_user_id2 = {tuple[0] for tuple in itemsPerUser[user_id2] if tuple[0] != item_id}\n",
    "                similarities.append(sim_func(simset_user_id, simset_user_id2))\n",
    "            elif sim_func == cosine_sim_binary:\n",
    "                if denom_over_all is None: denom_over_all = True\n",
    "                simset_user_id = {tuple[0] for tuple in itemsPerUser[user_id] if tuple[0] != item_id}\n",
    "                simset_user_id2 = {tuple[0] for tuple in itemsPerUser[user_id2] if tuple[0] != item_id}\n",
    "                similarities.append(sim_func(simset_user_id, simset_user_id2, denom_over_all))\n",
    "            elif sim_func == cosine_sim:\n",
    "                if denom_over_all is None: denom_over_all = True\n",
    "                simset_user_id = {tuple for tuple in itemsPerUser[user_id] if tuple[0] != item_id}\n",
    "                simset_user_id2 = {tuple for tuple in itemsPerUser[user_id2] if tuple[0] != item_id}\n",
    "                similarities.append(sim_func(simset_user_id, simset_user_id2, denom_over_all))\n",
    "            elif sim_func == pearson_sim:\n",
    "                simset_user_id = {(tuple, itemAverages[tuple[0]]) for tuple in itemsPerUser[user_id] if tuple[0] != item_id}\n",
    "                simset_user_id2 = {(tuple, itemAverages[tuple[0]]) for tuple in itemsPerUser[user_id2] if tuple[0] != item_id}\n",
    "                similarities.append(sim_func(simset_user_id, simset_user_id2))\n",
    "            else:\n",
    "                # Sim function not programmed\n",
    "                print(\"Invalid sim_func\")\n",
    "                return None\n",
    "            values.append(value - userAverages[user_id2])\n",
    "    else:\n",
    "        # Predict user_id's rating of item_id based on a weighted combination of how other users who\n",
    "        # rated item_id rated other items\n",
    "        # if item_id not in itemAverages: return meanValue # Skip keys that are not in the dict\n",
    "        avg_value = itemAverages[item_id]\n",
    "        for item_id2,value in itemsPerUser[user_id]:\n",
    "            if item_id2 == item_id: continue\n",
    "            # if item_id2 not in itemAverages: continue # Skip keys that are not in the dict\n",
    "            if sim_func == jaccard_sim:\n",
    "                simset_item_id = {tuple[0] for tuple in usersPerItem[item_id] if tuple[0] != user_id}\n",
    "                simset_item_id2 = {tuple[0] for tuple in usersPerItem[item_id2] if tuple[0] != user_id}\n",
    "                similarities.append(sim_func(simset_item_id, simset_item_id2))\n",
    "            elif sim_func == cosine_sim_binary:\n",
    "                if denom_over_all is None: denom_over_all = True\n",
    "                simset_item_id = {tuple[0] for tuple in usersPerItem[item_id] if tuple[0] != user_id}\n",
    "                simset_item_id2 = {tuple[0] for tuple in usersPerItem[item_id2] if tuple[0] != user_id}\n",
    "                similarities.append(sim_func(simset_item_id, simset_item_id2, denom_over_all))\n",
    "            elif sim_func == cosine_sim:\n",
    "                if denom_over_all is None: denom_over_all = True\n",
    "                simset_item_id = {tuple for tuple in usersPerItem[item_id] if tuple[0] != user_id}\n",
    "                simset_item_id2 = {tuple for tuple in usersPerItem[item_id2] if tuple[0] != user_id}\n",
    "                similarities.append(sim_func(simset_item_id, simset_item_id2, denom_over_all))\n",
    "            elif sim_func == pearson_sim:\n",
    "                simset_item_id = {(tuple, userAverages[tuple[0]]) for tuple in usersPerItem[item_id] if tuple[0] != user_id}\n",
    "                simset_item_id2 = {(tuple, userAverages[tuple[0]]) for tuple in usersPerItem[item_id2] if tuple[0] != user_id}\n",
    "                similarities.append(sim_func(simset_item_id, simset_item_id2))\n",
    "            else:\n",
    "                # Sim function not programmed\n",
    "                print(\"Invalid sim_func\")\n",
    "                return None\n",
    "            values.append(value - itemAverages[item_id2])\n",
    "    # Edge case 2: Return global mean value if there are no similar items\n",
    "    if np.sum(similarities) == 0: return meanValue\n",
    "\n",
    "    numerator = np.sum([value*sim for value,sim in zip(values, similarities)])\n",
    "    denominator = np.sum(similarities)\n",
    "    output = avg_value + (numerator / denominator)\n",
    "    if output < value_bounds[0]: return value_bounds[0]\n",
    "    if output > value_bounds[1]: return value_bounds[1]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c2fbe9-6f5a-4398-a0f4-f7884d1ebadd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### CUSTOM LATENT FACTOR MODEL\n",
    "######################################\n",
    "def initialize_params(meanValue, itemsPerUser, usersPerItem, k, init_bounds):\n",
    "    \"\"\"\n",
    "    Return the initialized parameters\n",
    "    \"\"\"\n",
    "    users, items = [key for key in itemsPerUser.keys()], [key for key in usersPerItem.keys()]\n",
    "    user_count, item_count = len(users), len(items)\n",
    "    lower, upper = init_bounds\n",
    "    alpha = meanValue\n",
    "    user_bias = dict(zip(users, [random.uniform(lower,upper) for i in range(user_count)]))\n",
    "    item_bias = dict(zip(items, [random.uniform(lower,upper) for i in range(item_count)]))\n",
    "    user_gamma = dict(zip(users, [np.array([random.uniform(lower,upper) for ki in range(k)]) for i in range(user_count)]))\n",
    "    item_gamma = dict(zip(items, [np.array([random.uniform(lower,upper) for ki in range(k)]) for i in range(item_count)]))\n",
    "\n",
    "    return (alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem)\n",
    "\n",
    "def get_lfm_defaults(theta):\n",
    "    \"\"\"\n",
    "    Calculate average values/vectors to use as default values for the latent factor predictions\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem = theta\n",
    "    avg_params = {}\n",
    "    avg_params[\"avg_user_bias\"] = np.mean(np.array([i for i in user_bias.values()]))\n",
    "    avg_params[\"avg_item_bias\"] = np.mean(np.array([i for i in item_bias.values()]))\n",
    "    avg_params[\"avg_user_gamma\"] = np.mean(np.array([values for values in user_gamma.values()]), axis=0)\n",
    "    avg_params[\"avg_item_gamma\"] = np.mean(np.array([values for values in item_gamma.values()]), axis=0)\n",
    "\n",
    "    return avg_params\n",
    "\n",
    "def get_lfm_terms(u_id, item_id, theta, avg_params):\n",
    "    \"\"\"\n",
    "    For an input u_id and item_id, return all bias/gamma terms\n",
    "    If one is known but the other is not, use an average parameter if available\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem = theta\n",
    "    u_bias, i_bias, gamma_term = 0, 0, 0\n",
    "    \n",
    "    # user bias\n",
    "    if u_id in user_bias: u_bias = user_bias[u_id]\n",
    "    # item bias\n",
    "    if item_id in item_bias: i_bias = item_bias[item_id]\n",
    "    # user/item gamma\n",
    "    if (u_id in user_gamma) and (item_id in item_gamma):\n",
    "        gamma_term = np.dot(user_gamma[u_id], item_gamma[item_id])\n",
    "\n",
    "    return (u_bias, i_bias, gamma_term)\n",
    "\n",
    "def predict_latent_factor(u_id, item_id, theta, value_bounds = None, avg_params = None):\n",
    "    \"\"\"\n",
    "    Return the prediction based on u_id and item_id\n",
    "    If u_id is not in itemsPerUser --> Use the average gamma vector\n",
    "    Repeat this process for if item_id is not in usersPerItem\n",
    "\n",
    "    Bound the output by the min and max of the possible values\n",
    "    (ex: Model shouldn't exceed 5 when the scale is 5)\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem = theta\n",
    "    if value_bounds is None: value_bounds = (-np.inf, np.inf)\n",
    "    for i,(gamma_vec) in enumerate(user_gamma.values()):\n",
    "        if i > 0: break\n",
    "        k = len(gamma_vec)\n",
    "\n",
    "    # Reg user/item terms\n",
    "    u_bias, i_bias, gamma_term = get_lfm_terms(u_id, item_id, theta, avg_params)\n",
    "    value = alpha + u_bias + i_bias + gamma_term\n",
    "    # print(value)\n",
    "    if value < value_bounds[0]: return value_bounds[0]\n",
    "    if value > value_bounds[1]: return value_bounds[1]\n",
    "\n",
    "    return value\n",
    "\n",
    "def get_cost_mse(theta, lambda_bias, lambda_gamma, Xtrain, ytrain, k, value_bounds=None):\n",
    "    \"\"\"\n",
    "    Calculate the cost for the given theta parameters\n",
    "    Xtrain must be of form [(user, item), (user, item), ...]\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem = theta\n",
    "    # Predict using the current theta values\n",
    "    predictions = np.array([predict_latent_factor(tuple[0], tuple[1], theta, value_bounds) for tuple in Xtrain])\n",
    "    # Get training mse\n",
    "    mse = get_MSE(predictions, ytrain)\n",
    "    # Calculate SSE + regularization\n",
    "    cost = get_SSE(predictions, ytrain)\n",
    "    cost += lambda_bias * np.sum(np.array([val**2 for val in user_bias.values()]))\n",
    "    cost += lambda_bias * np.sum(np.array([val**2 for val in item_bias.values()]))\n",
    "    cost += lambda_gamma * np.sum(np.array([np.dot(gam, gam) for gam in user_gamma.values()]))\n",
    "    cost += lambda_gamma * np.sum(np.array([np.dot(gam, gam) for gam in item_gamma.values()]))\n",
    "\n",
    "    return (cost, mse)\n",
    "\n",
    "def get_batches(x, n):\n",
    "    limit = len(x)\n",
    "    for i in range(0, limit, n):\n",
    "        yield x[i:min(i + n, limit)]\n",
    "\n",
    "def update_alpha(theta, Xtrain, ytrain):\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem = theta\n",
    "    alpha_sum = 0\n",
    "    n_rows = len(ytrain)\n",
    "    row_sums = np.zeros((n_rows, 4))\n",
    "    row_sums[:,0] = np.array(ytrain)\n",
    "    for i,(u_id, item_id) in enumerate(Xtrain):\n",
    "        row_sums[i,1:] = np.array([-user_bias[u_id], -item_bias[item_id], -np.dot(user_gamma[u_id], item_gamma[item_id])])\n",
    "    alpha = row_sums.sum() / n_rows\n",
    "    return alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem\n",
    "\n",
    "def update_bias(theta, Xtrain, ytrain, lambda_bias, update_type):\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem = theta\n",
    "    if update_type == \"user\":\n",
    "        dict1, bias1, gamma1 = itemsPerUser, user_bias, user_gamma\n",
    "        dict2, bias2, gamma2 = usersPerItem, item_bias, item_gamma\n",
    "    else:\n",
    "        dict1, bias1, gamma1 = usersPerItem, item_bias, item_gamma\n",
    "        dict2, bias2, gamma2 = itemsPerUser, user_bias, user_gamma\n",
    "    for id1 in dict1:\n",
    "        n_rows = len(dict1[id1])\n",
    "        row_sums = np.zeros((n_rows, 4))\n",
    "        row_sums[:,1] = -alpha\n",
    "        for i,(id2,rating) in enumerate(dict1[id1]):\n",
    "            row_sums[i,0] = rating\n",
    "            row_sums[i,2:] = np.array([-bias2[id2], -np.dot(gamma1[id1], gamma2[id2])])\n",
    "        bias1[id1] = row_sums.sum() / (n_rows + lambda_bias)\n",
    "    if update_type == \"user\": \n",
    "        user_bias = bias1.copy()\n",
    "    else: \n",
    "        item_bias = bias1.copy()\n",
    "    return alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem\n",
    "\n",
    "def update_bias_and_gamma(theta, Xtrain, ytrain, lambda_bias, lambda_gamma, update_type, k):\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem = theta\n",
    "    if update_type == \"user\":\n",
    "        dict1, bias1, gamma1 = itemsPerUser, user_bias.copy(), user_gamma.copy()\n",
    "        dict2, bias2, gamma2 = usersPerItem, item_bias.copy(), item_gamma.copy()\n",
    "    else:\n",
    "        dict1, bias1, gamma1 = usersPerItem, item_bias.copy(), item_gamma.copy()\n",
    "        dict2, bias2, gamma2 = itemsPerUser, user_bias.copy(), user_gamma.copy()\n",
    "    for id1 in dict1:\n",
    "        n_rows = len(dict1[id1])\n",
    "        row_sums = np.zeros((n_rows, 4))\n",
    "        row_sums[:,1] = -alpha\n",
    "        #\n",
    "        row_sums2 = np.zeros((n_rows, 4))\n",
    "        row_sums2[:,1] = -alpha\n",
    "        gamma2s = np.zeros((n_rows, k))\n",
    "        gamma2s_sqrd = np.zeros((n_rows, k))\n",
    "        for i,(id2,rating) in enumerate(dict1[id1]):\n",
    "            row_sums[i,0] = rating\n",
    "            row_sums[i,2:] = np.array([-bias2[id2], -np.dot(gamma1[id1], gamma2[id2])])\n",
    "            row_sums2[i,0] = rating\n",
    "            row_sums2[i,3] = -bias2[id2]\n",
    "            gamma2s[i,:] = gamma2[id2]\n",
    "            gamma2s_sqrd[i,:] = (gamma2[id2]**2)\n",
    "        bias1[id1] = row_sums.sum() / (n_rows + lambda_bias)\n",
    "        # Update gamma1\n",
    "        row_sums2[:,2] = np.full(n_rows, -bias1[id1])\n",
    "        sum_term = np.repeat(row_sums2.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "        gamma1[id1] = (sum_term * gamma2s).sum(axis=0) / (gamma2s_sqrd.sum(axis=0) + lambda_gamma)\n",
    "    if update_type == \"user\": \n",
    "        user_bias = bias1.copy()\n",
    "        user_gamma = gamma1.copy()\n",
    "    else: \n",
    "        item_bias = bias1.copy()\n",
    "        item_gamma = gamma1.copy()\n",
    "    return alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem\n",
    "\n",
    "def update_params(theta, lambda_bias, lambda_gamma, Xtrain, ytrain):\n",
    "    \"\"\"\n",
    "    Update parameters based on how well they predict in their CURRENT states\n",
    "    Coordinate descent instead of gradient descent for faster convergence\n",
    "    ###\n",
    "    Latent factor model\n",
    "         Fix user_gamma --> iterate and update alpha, user_bias, item_gamma\n",
    "         Fix item_gamma --> iterate and update alpha, user_bias, user_gamma\n",
    "    Fix user_gamma... Fix item_gamma...\n",
    "    Repeat the above until model have converged\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, itemsPerUser, usersPerItem = theta\n",
    "\n",
    "    # Calculate the new alpha\n",
    "    t0 = time.time()\n",
    "    theta = update_alpha(theta, Xtrain, ytrain)\n",
    "    # print(f\"   Alpha Complete --> Time elapsed: {time.time() - t0}\")\n",
    "    # user bias and gamma\n",
    "    t0 = time.time()\n",
    "    theta = update_bias_and_gamma(theta, Xtrain, ytrain, lambda_bias, lambda_gamma, \"user\", k)\n",
    "    # print(f\"   User Bias and Gamma Complete --> Time elapsed: {time.time() - t0}\")\n",
    "    # item bias and gamma\n",
    "    t0 = time.time()\n",
    "    theta = update_bias_and_gamma(theta, Xtrain, ytrain, lambda_bias, lambda_gamma, \"item\", k)\n",
    "    # print(f\"   Item Bias and Gamma Complete --> Time elapsed: {time.time() - t0}\")\n",
    "        \n",
    "        # if cycle % 2:\n",
    "        #     # item bias\n",
    "        #     t0 = time.time()\n",
    "        #     theta = update_bias(theta, Xtrain, ytrain, lambda_bias, \"item\")\n",
    "        #     print(f\"   Item Biases Complete --> Time elapsed: {time.time() - t0}\")\n",
    "        #     # user bias and gamma\n",
    "        #     t0 = time.time()\n",
    "        #     theta = update_bias_and_gamma(theta, Xtrain, ytrain, lambda_bias, lambda_gamma, \"user\", k)\n",
    "        #     print(f\"   User Bias and Gamma Complete --> Time elapsed: {time.time() - t0}\")\n",
    "        # else:\n",
    "        #     # user bias\n",
    "        #     t0 = time.time()\n",
    "        #     theta = update_bias(theta, Xtrain, ytrain, lambda_bias, \"user\")\n",
    "        #     print(f\"   User Bias Complete --> Time elapsed: {time.time() - t0}\")\n",
    "        #     # item bias and gamma\n",
    "        #     t0 = time.time()\n",
    "        #     theta = update_bias_and_gamma(theta, Xtrain, ytrain, lambda_bias, lambda_gamma, \"item\", k)\n",
    "        #     print(f\"   Item Bias and Gamma Complete --> Time elapsed: {time.time() - t0}\")\n",
    "\n",
    "    return theta\n",
    "\n",
    "def fit_parameters(Xtrain, ytrain, theta, ep=0.0005, iter_limit=200, quiet=True, value_bounds=(-np.inf, np.inf), mini_batch=False, n_mini_batch=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Fit the parameters until convergence (when difference in cost is less than ep)\n",
    "    Arguments packed into **kwargs:\n",
    "    lambda_bias --> Regularization parameter for the user/item biases\n",
    "    lambda_gamma --> Regularization parameter for the user/item gamma matrix\n",
    "    k --> Number of latent parameters to use per user/item vector\n",
    "    ep --> The threshold for early stopping between mse checks\n",
    "    iter_limit --> The maximum number of iterations allowed\n",
    "    value_bounds --> The expected range of values expected to be outputted by the model\n",
    "    check_every --> Calculate the cost/mse whenever iter_count is perfectly divisible by check_every (ex: check_every=10 means check every 10 iterations)\n",
    "    \"\"\"\n",
    "    last_mse, last_cost = np.inf, np.inf\n",
    "    best_params = [theta, last_mse, last_cost]\n",
    "    epoch_count = 0\n",
    "    if mini_batch:\n",
    "        if n_mini_batch is None: n_mini_batch = 5\n",
    "        batch_size = math.ceil(len(ytrain) / n_mini_batch)\n",
    "\n",
    "    if not quiet:\n",
    "        print(\"Fitting parameters...\\n-----\")\n",
    "    while True:\n",
    "        epoch_count += 1\n",
    "        ### Update theta\n",
    "        t_theta = time.time()\n",
    "        if mini_batch:\n",
    "            X_y = list(zip(Xtrain, ytrain))\n",
    "            random.shuffle(X_y)\n",
    "            Xtrain, ytrain = zip(*X_y)\n",
    "            for X_batch, y_batch in zip(get_batches(Xtrain, batch_size), get_batches(ytrain, batch_size)):\n",
    "                theta = update_params(theta, lambda_bias, lambda_gamma, X_batch, y_batch)\n",
    "        else:\n",
    "            theta = update_params(theta, lambda_bias, lambda_gamma, Xtrain, ytrain)\n",
    "\n",
    "        ### Compute cost and output results as needed\n",
    "        cost, mse = get_cost_mse(theta, lambda_bias, lambda_gamma, Xtrain, ytrain, k)\n",
    "        if not quiet:\n",
    "            print(f\"-----\\nEpoch {epoch_count}: Cost = {cost}, Train MSE = {mse}, Time Elapsed: {time.time() - t_theta}\\n-----\")\n",
    "\n",
    "        ### Save current params as best_params if the mse is less than the last mse\n",
    "        if mse < best_params[1]:\n",
    "            best_params = [theta, mse, cost]\n",
    "        ### Check if cost is too high - sometimes the algorithm diverges\n",
    "        if mse > 5000:\n",
    "            theta, mse, cost = best_params\n",
    "            print(f\"Training MSE too high: Best Train MSE = {best_params[1]}\")\n",
    "            break\n",
    "        ### Early stop if the ep condition is met\n",
    "        if (abs(last_mse - mse) > ep):\n",
    "            last_mse, last_cost = mse, cost\n",
    "        else:\n",
    "            print(f\"Convergence after {epoch_count} epochs: Cost = {cost}, Train MSE = {mse}\")\n",
    "            break\n",
    "        ### If the iteration limit is reached, stop and return the best parameters\n",
    "        if epoch_count > iter_limit:\n",
    "            theta, mse, cost = best_params\n",
    "            print(f\"Iteration limit reached after {epoch_count} epochs: Best Train MSE = {best_params[1]}\")\n",
    "            break\n",
    "\n",
    "    return (theta, cost, mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34fc4ebd-295d-4f51-a737-ea6c022bb68d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Popularity-based Baseline\n",
    "######################################\n",
    "\n",
    "# Rank the items in the dataset based on popularity --> If the item is popular, predict 1, else 0\n",
    "\n",
    "def get_item_popularities(df, col):\n",
    "    # Define popularity: The items which have the highest playtime per person ratios weighted by number of plays\n",
    "    item_playCounts = df.groupby(\"item_id\").agg(\"sum\")[col]\n",
    "    item_playCountsBinary = df.groupby(\"item_id\").agg(\"sum\")[\"playtime_binary\"]\n",
    "    item_userCounts = df.groupby(\"item_id\").count()[\"user_id\"]\n",
    "\n",
    "    item_playsPerUser = (item_playCountsBinary / item_userCounts).sort_values(ascending=True)\n",
    "    # item_playsPerUser = (item_playCounts).sort_values(ascending=True)\n",
    "\n",
    "    return pd.DataFrame({\"pop_values\":item_playsPerUser, \"pop_sums\":item_playsPerUser.cumsum()})\n",
    "\n",
    "def get_most_popular_items(df, cutoff):\n",
    "    pop_cutoff = df[\"pop_values\"].max() * cutoff\n",
    "\n",
    "    return list(df[df[\"pop_values\"] > pop_cutoff].index)\n",
    "\n",
    "def predict_by_popularity(X, pop_items):\n",
    "    if isinstance(X, tuple):\n",
    "        if X[1] in pop_items: return 1\n",
    "        else: return 0\n",
    "    else:\n",
    "        output = pd.Series([0 for i in range(len(X))])\n",
    "        output[X[\"item_id\"].isin(pop_items)] = 1\n",
    "        return output\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71797786-1619-4ee2-9a2d-e01266368bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OTHER HELPFUL FUNCTIONS\n",
    "######################################\n",
    "def get_rec_structs(train_data):\n",
    "    \"\"\"\n",
    "    Extract stats used for creating the classifier features\n",
    "    Input is (user_id, item_id, value), ...\n",
    "    Typically value is rating, but can be other things (e.g., hours played)\n",
    "\n",
    "    itemsPerUser: Records each item in the training set that each user interacted with (along with the corresponding value)\n",
    "    usersPerItem: Records each user in the training set that each item interacted with (along with the corresponding value)\n",
    "    valueDict: Records the value for each (user, item) tuple\n",
    "    userAverages: Gives the average value for each user\n",
    "    itemAverages: Gives the average value for each item\n",
    "\n",
    "    \"\"\"\n",
    "    ### Record which items each user interacted with and which users interacted with which item\n",
    "    itemsPerUser = defaultdict(list)\n",
    "    usersPerItem = defaultdict(list)\n",
    "    userItemDict, itemUserDict = {}, {}\n",
    "    for u,b,v in train_data:\n",
    "        itemsPerUser[u].append((b,v))\n",
    "        usersPerItem[b].append((u,v))\n",
    "        userItemDict[(u,b)] = v\n",
    "        itemUserDict[(b,u)] = v\n",
    "\n",
    "    ### Calculate user and item average ratings\n",
    "    userAverages = {}\n",
    "    itemAverages = {}\n",
    "    for u,tuples in itemsPerUser.items():\n",
    "        values = [value for item,value in tuples]\n",
    "        # values = [value for item,value in tuples if value != 0]\n",
    "        # if len(values) == 0: continue\n",
    "        userAverages[u] = sum(values) / len(values)\n",
    "    for i,tuples in usersPerItem.items():\n",
    "        values = [value for user,value in tuples]\n",
    "        # values = [value for user,value in tuples if value != 0]\n",
    "        # if len(values) == 0: continue\n",
    "        itemAverages[i] = sum(values) / len(values)\n",
    "\n",
    "    rec_structs = {\"itemsPerUser\":itemsPerUser,\n",
    "                  \"usersPerItem\":usersPerItem,\n",
    "                  \"userItemDict\":userItemDict,\n",
    "                  \"itemUserDict\":itemUserDict,\n",
    "                  \"userAverages\":userAverages,\n",
    "                  \"itemAverages\":itemAverages,}\n",
    "\n",
    "    return rec_structs\n",
    "\n",
    "\n",
    "def unpack_rec_structs(rec_structs):\n",
    "    \"\"\"\n",
    "    Take the input recommender_structs and return the itemized contents\n",
    "    Not used in this assignment\n",
    "    \"\"\"\n",
    "    itemsPerUser = rec_structs[\"itemsPerUser\"]\n",
    "    usersPerItem = rec_structs[\"usersPerItem\"]\n",
    "    userItemDict = rec_structs[\"userItemDict\"]\n",
    "    itemUserDict = rec_structs[\"itemUserDict\"]\n",
    "    userAverages = rec_structs[\"userAverages\"]\n",
    "    itemAverages = rec_structs[\"itemAverages\"]\n",
    "\n",
    "    return (itemsPerUser, usersPerItem, userItemDict, itemUserDict, userAverages, itemAverages)\n",
    "\n",
    "def pd_get_rec_structs(df, user_col, item_col, val_col, user_limit=None, item_limit=None, only_nonzero=None):\n",
    "    \"\"\"\n",
    "    Like get_rec_structs(), but is customized for the pandas library\n",
    "    Extract stats used for creating the classifier features\n",
    "    Input is df which has at least a user_col, item_col, val_col, ...\n",
    "    Typically value is rating, but can be other things (e.g., hours played)\n",
    "\n",
    "    itemsPerUser: Records each item in the training set that each user interacted with (along with the corresponding value)\n",
    "    usersPerItem: Records each user in the training set that each item interacted with (along with the corresponding value)\n",
    "    valueDict: Records the value for each (user, item) tuple\n",
    "    userAverages: Gives the average value for each user\n",
    "    itemAverages: Gives the average value for each item\n",
    "\n",
    "    user/item limit allows for estimating with less data to save time\n",
    "    only_nonzero: Only track users/items that have non-zero values\n",
    "    \"\"\"\n",
    "    if only_nonzero is None: only_nonzero = False\n",
    "    df2 = df.copy()\n",
    "    df2[\"itemsPerUser\"] = list(zip(df2[item_col], df2[val_col]))\n",
    "    df2[\"usersPerItem\"] = list(zip(df2[user_col], df2[val_col]))\n",
    "    df2[\"user_item\"] = list(zip(df2[user_col], df2[item_col]))\n",
    "    #\n",
    "    if user_limit is None: user_limit = len(df)\n",
    "    if item_limit is None: item_limit = len(df)\n",
    "    # Limit data to only_nonzero if necessary\n",
    "    if only_nonzero: df2 = df2[df2[val_col] > 0]\n",
    "    itemsPerUser = df2.groupby(user_col)[\"itemsPerUser\"].apply(lambda x: list(x)[:item_limit]).to_dict()\n",
    "    usersPerItem = df2.groupby(item_col)[\"usersPerItem\"].apply(lambda x: list(x)[:user_limit]).to_dict()\n",
    "\n",
    "    valueDict = df2[[\"user_item\", val_col]].set_index(\"user_item\", drop=True).to_dict()[val_col]\n",
    "    userAverages = df2.groupby(user_col)[val_col].mean().to_dict()\n",
    "    itemAverages = df2.groupby(item_col)[val_col].mean().to_dict()\n",
    "    meanValue = np.mean([val for val in valueDict.values()])\n",
    "\n",
    "    return itemsPerUser, usersPerItem, valueDict, userAverages, itemAverages, meanValue\n",
    "\n",
    "### Functions to read files (From homework stubs)\n",
    "def readGz(path):\n",
    "    output = []\n",
    "    for l in gzip.open(path, mode = 'rt', encoding = \"utf-8\"):\n",
    "        output.append(eval(l))\n",
    "    return output\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r\n",
    "\n",
    "def plot_lfm_results(results, max_results=None):\n",
    "    results_to_pd = []\n",
    "    for result in results:\n",
    "        results_to_pd.append({\"MSE\":result[0], \"k\":result[1][0], \\\n",
    "                              \"lambda_bias\":result[1][1], \"lambda_gamma\":result[1][2]})\n",
    "    \n",
    "    results_df = pd.DataFrame(results_to_pd)\n",
    "    results_df\n",
    "    \n",
    "    n_results = len(results_df)\n",
    "    n_k = len(results_df.k.unique())\n",
    "    n_lambda_gammas = len(results_df.lambda_gamma.unique())\n",
    "    n_lambda_biases = len(results_df.lambda_bias.unique())\n",
    "    cmap = cm.viridis\n",
    "    norm = colors.Normalize()\n",
    "    if max_results is None:\n",
    "        max_results = np.inf\n",
    "    \n",
    "    if n_k == 1: n_subplot_rows = 2\n",
    "    else: n_subplot_rows = n_k\n",
    "    fig,axes = plt.subplots(n_subplot_rows, 1, figsize=(10, 5*n_subplot_rows))\n",
    "    ax1_count = 0\n",
    "    \n",
    "    for i,(k,ax) in enumerate(zip(results_df.k.unique(), axes.flatten())):\n",
    "        plot_data = results_df[results_df[\"k\"] == k]\n",
    "        plot_data[plot_data[\"MSE\"] > max_results] = max_results\n",
    "        mses, lambda_biases, lambda_gammas = plot_data.MSE, plot_data.lambda_bias, plot_data.lambda_gamma\n",
    "        best_mse = mses.min()\n",
    "        plot_valid_mses = np.flip(mses.to_numpy().reshape((n_lambda_gammas, n_lambda_biases)), axis=0)\n",
    "        extent = [np.min(lambda_biases), np.max(lambda_biases), np.min(lambda_gammas), np.max(lambda_gammas)]\n",
    "        plot0 = ax.imshow(plot_valid_mses.T, extent=extent)\n",
    "    \n",
    "        ax.set(title=f\"MSE at k = {k}\\nBest MSE = {best_mse}\", xlabel=\"lambda_bias\", ylabel=\"lambda_gamma\", \\\n",
    "               xlim=[np.min(lambda_biases), np.max(lambda_biases)], ylim=[np.min(lambda_gammas), np.max(lambda_gammas)])\n",
    "        plt.colorbar(plot0, cmap=cmap, norm=norm, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dc0f3ef-c01d-4471-85ce-66c269fe3184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 516 ms\n",
      "Wall time: 526 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u2194</td>\n",
       "      <td>9350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u7381</td>\n",
       "      <td>219890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u8653</td>\n",
       "      <td>238210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u7474</td>\n",
       "      <td>2430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u4426</td>\n",
       "      <td>11200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676905</th>\n",
       "      <td>u6686</td>\n",
       "      <td>110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676906</th>\n",
       "      <td>u740</td>\n",
       "      <td>49600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676907</th>\n",
       "      <td>u11882</td>\n",
       "      <td>409720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676908</th>\n",
       "      <td>u3544</td>\n",
       "      <td>221380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676909</th>\n",
       "      <td>u10215</td>\n",
       "      <td>322330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676910 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id\n",
       "0        u2194     9350\n",
       "1        u7381   219890\n",
       "2        u8653   238210\n",
       "3        u7474     2430\n",
       "4        u4426    11200\n",
       "...        ...      ...\n",
       "676905   u6686   110800\n",
       "676906    u740    49600\n",
       "676907  u11882   409720\n",
       "676908   u3544   221380\n",
       "676909  u10215   322330\n",
       "\n",
       "[676910 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "### Load review data\n",
    "seed = 100\n",
    "\n",
    "minmax_scalers = pd.read_csv(\"data/minmax_scalers.csv\")\n",
    "# test_df_results = pd.read_csv(\"data/test_df_results.csv\")\n",
    "\n",
    "# Train/valid/train2/test\n",
    "train_filepath = \"data/train_df.csv\"\n",
    "valid_filepath = \"data/valid_df.csv\"\n",
    "train2_filepath = \"data/train_df2.csv\"\n",
    "test_filepath = \"data/test_df.csv\"\n",
    "train_df = pd.read_csv(train_filepath)\n",
    "valid_df = pd.read_csv(valid_filepath)\n",
    "train_df2 = pd.read_csv(train2_filepath)\n",
    "test_df = pd.read_csv(test_filepath)\n",
    "\n",
    "# Further split the data into X/y pairs for convenience\n",
    "class_col = \"playtime_log\" # This what we want to predict\n",
    "X_train, y_train = train_df[[\"user_id\", \"item_id\"]], train_df[class_col]\n",
    "X_valid, y_valid = valid_df[[\"user_id\", \"item_id\"]], valid_df[class_col]\n",
    "X_test, y_test = test_df[[\"user_id\", \"item_id\"]], test_df[class_col]\n",
    "X_train2, y_train2 = train_df2[[\"user_id\", \"item_id\"]], train_df2[class_col]\n",
    "\n",
    "X_train2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8723e9b-e2aa-4422-bdcb-8f548af6dbc0",
   "metadata": {},
   "source": [
    "### LATENT FACTOR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b176a2-3077-4d0e-99a8-0bb676cf197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1bb1ce7-79aa-46d5-a4e8-dff6efb4859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14540\n",
      "9151\n",
      "541528\n",
      "14540\n",
      "9151\n",
      "3.3104654998952894\n"
     ]
    }
   ],
   "source": [
    "##### itemsPerUser\n",
    "itemsPerUser, usersPerItem, valueDict, userAverages, itemAverages, meanValue = pd_get_rec_structs(train_df, \"user_id\", \"item_id\", class_col)\n",
    "\n",
    "print(len(itemsPerUser))\n",
    "print(len(usersPerItem))\n",
    "print(len(valueDict))\n",
    "print(len(userAverages))\n",
    "print(len(itemAverages))\n",
    "print(meanValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0796ce7b-5ee9-430e-baed-98a2eadeabdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence after 13 epochs: Cost = 2196174.599932365, Train MSE = 4.0555143961759414\n",
      "#-----\n",
      "Iteration 1/100: Valid MSE = 5.421893025540095, Combo Parameters = (1, 0.0, 0.0)\n",
      "Time elapsed = 1.0 min and 15.771487474441528s (75.77148747444153s)\n",
      "Convergence after 11 epochs: Cost = 2196484.7140804166, Train MSE = 4.0264259162504175\n",
      "#-----\n",
      "Iteration 2/100: Valid MSE = 4.147657075939483, Combo Parameters = (1, 0.0, 0.6666666666666666)\n",
      "Time elapsed = 1.0 min and 10.150934934616089s (70.15093493461609s)\n",
      "Convergence after 10 epochs: Cost = 2221841.200034994, Train MSE = 4.064449609454593\n",
      "#-----\n",
      "Iteration 3/100: Valid MSE = 4.181269820582037, Combo Parameters = (1, 0.0, 1.3333333333333333)\n",
      "Time elapsed = 1.0 min and 6.528456211090088s (66.52845621109009s)\n",
      "Convergence after 9 epochs: Cost = 2227933.4395176754, Train MSE = 4.065769581667649\n",
      "#-----\n",
      "Iteration 4/100: Valid MSE = 4.181250347599356, Combo Parameters = (1, 0.0, 2.0)\n",
      "Time elapsed = 0.0 min and 58.096423387527466s (58.096423387527466s)\n",
      "Convergence after 8 epochs: Cost = 2220223.6627953136, Train MSE = 4.046432146819791\n",
      "#-----\n",
      "Iteration 5/100: Valid MSE = 4.159416835482152, Combo Parameters = (1, 0.0, 2.6666666666666665)\n",
      "Time elapsed = 0.0 min and 51.30438423156738s (51.30438423156738s)\n",
      "Convergence after 9 epochs: Cost = 2226981.7374928053, Train MSE = 4.053279845853233\n",
      "#-----\n",
      "Iteration 6/100: Valid MSE = 4.16146237739849, Combo Parameters = (1, 0.0, 3.333333333333333)\n",
      "Time elapsed = 0.0 min and 54.14263296127319s (54.14263296127319s)\n",
      "Convergence after 10 epochs: Cost = 2234917.5065349615, Train MSE = 4.060765888063739\n",
      "#-----\n",
      "Iteration 7/100: Valid MSE = 4.171693298441471, Combo Parameters = (1, 0.0, 4.0)\n",
      "Time elapsed = 1.0 min and 0.21068859100341797s (60.21068859100342s)\n",
      "Convergence after 9 epochs: Cost = 2251665.919156788, Train MSE = 4.088778960407456\n",
      "#-----\n",
      "Iteration 8/100: Valid MSE = 4.192938179754505, Combo Parameters = (1, 0.0, 4.666666666666666)\n",
      "Time elapsed = 0.0 min and 54.464815616607666s (54.464815616607666s)\n",
      "Convergence after 7 epochs: Cost = 2259394.5657527777, Train MSE = 4.100446924288524\n",
      "#-----\n",
      "Iteration 9/100: Valid MSE = 4.199450895728554, Combo Parameters = (1, 0.0, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 41.96526265144348s (41.96526265144348s)\n",
      "Convergence after 8 epochs: Cost = 2245379.4762011273, Train MSE = 4.06854736205908\n",
      "#-----\n",
      "Iteration 10/100: Valid MSE = 4.1717429834690885, Combo Parameters = (1, 0.0, 6.0)\n",
      "Time elapsed = 0.0 min and 49.95041584968567s (49.95041584968567s)\n",
      "Convergence after 9 epochs: Cost = 2286659.122289128, Train MSE = 4.1806151747280955\n",
      "#-----\n",
      "Iteration 11/100: Valid MSE = 5.256814350883769, Combo Parameters = (1, 0.6666666666666666, 0.0)\n",
      "Time elapsed = 0.0 min and 56.46465253829956s (56.46465253829956s)\n",
      "Convergence after 11 epochs: Cost = 2226881.1854521767, Train MSE = 4.044615186270829\n",
      "#-----\n",
      "Iteration 12/100: Valid MSE = 4.159099081128472, Combo Parameters = (1, 0.6666666666666666, 0.6666666666666666)\n",
      "Time elapsed = 1.0 min and 10.048963785171509s (70.04896378517151s)\n",
      "Convergence after 9 epochs: Cost = 2236011.3115229057, Train MSE = 4.048856311692782\n",
      "#-----\n",
      "Iteration 13/100: Valid MSE = 4.164060133693579, Combo Parameters = (1, 0.6666666666666666, 1.3333333333333333)\n",
      "Time elapsed = 0.0 min and 54.53866648674011s (54.53866648674011s)\n",
      "Convergence after 9 epochs: Cost = 2238565.5076388274, Train MSE = 4.044735304149208\n",
      "#-----\n",
      "Iteration 14/100: Valid MSE = 4.156477008075843, Combo Parameters = (1, 0.6666666666666666, 2.0)\n",
      "Time elapsed = 0.0 min and 53.84010696411133s (53.84010696411133s)\n",
      "Convergence after 12 epochs: Cost = 2250004.5669118543, Train MSE = 4.060424400190746\n",
      "#-----\n",
      "Iteration 15/100: Valid MSE = 4.157950737373503, Combo Parameters = (1, 0.6666666666666666, 2.6666666666666665)\n",
      "Time elapsed = 1.0 min and 14.929185628890991s (74.92918562889099s)\n",
      "Convergence after 13 epochs: Cost = 2253925.7109108595, Train MSE = 4.059662222425314\n",
      "#-----\n",
      "Iteration 16/100: Valid MSE = 4.1607401300464035, Combo Parameters = (1, 0.6666666666666666, 3.333333333333333)\n",
      "Time elapsed = 1.0 min and 23.640808820724487s (83.64080882072449s)\n",
      "Convergence after 10 epochs: Cost = 2268448.736377025, Train MSE = 4.084353051974622\n",
      "#-----\n",
      "Iteration 17/100: Valid MSE = 4.187252429709311, Combo Parameters = (1, 0.6666666666666666, 4.0)\n",
      "Time elapsed = 1.0 min and 2.0870797634124756s (62.087079763412476s)\n",
      "Convergence after 9 epochs: Cost = 2278747.6369091216, Train MSE = 4.096396032519826\n",
      "#-----\n",
      "Iteration 18/100: Valid MSE = 4.191764693522772, Combo Parameters = (1, 0.6666666666666666, 4.666666666666666)\n",
      "Time elapsed = 0.0 min and 56.100149393081665s (56.100149393081665s)\n",
      "Convergence after 8 epochs: Cost = 2284184.4187160754, Train MSE = 4.103071764543972\n",
      "#-----\n",
      "Iteration 19/100: Valid MSE = 4.195936640765638, Combo Parameters = (1, 0.6666666666666666, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 48.07581090927124s (48.07581090927124s)\n",
      "Convergence after 8 epochs: Cost = 2297152.8631075383, Train MSE = 4.1262876389499406\n",
      "#-----\n",
      "Iteration 20/100: Valid MSE = 4.225758995526153, Combo Parameters = (1, 0.6666666666666666, 6.0)\n",
      "Time elapsed = 0.0 min and 47.940913915634155s (47.940913915634155s)\n",
      "Convergence after 9 epochs: Cost = 2302031.1968082897, Train MSE = 4.182247261949488\n",
      "#-----\n",
      "Iteration 21/100: Valid MSE = 7.141127882981046, Combo Parameters = (1, 1.3333333333333333, 0.0)\n",
      "Time elapsed = 0.0 min and 55.17776799201965s (55.17776799201965s)\n",
      "Convergence after 11 epochs: Cost = 2259070.5057444163, Train MSE = 4.077563410110169\n",
      "#-----\n",
      "Iteration 22/100: Valid MSE = 4.19058988624658, Combo Parameters = (1, 1.3333333333333333, 0.6666666666666666)\n",
      "Time elapsed = 1.0 min and 9.659486055374146s (69.65948605537415s)\n",
      "Convergence after 12 epochs: Cost = 2263681.377461517, Train MSE = 4.0736956035181855\n",
      "#-----\n",
      "Iteration 23/100: Valid MSE = 4.182536942733937, Combo Parameters = (1, 1.3333333333333333, 1.3333333333333333)\n",
      "Time elapsed = 1.0 min and 12.73663854598999s (72.73663854598999s)\n",
      "Convergence after 10 epochs: Cost = 2256186.1716462104, Train MSE = 4.051738255916878\n",
      "#-----\n",
      "Iteration 24/100: Valid MSE = 4.155639580505811, Combo Parameters = (1, 1.3333333333333333, 2.0)\n",
      "Time elapsed = 0.0 min and 57.7980854511261s (57.7980854511261s)\n",
      "Convergence after 9 epochs: Cost = 2281316.0952935577, Train MSE = 4.09094628297911\n",
      "#-----\n",
      "Iteration 25/100: Valid MSE = 4.196991162296007, Combo Parameters = (1, 1.3333333333333333, 2.6666666666666665)\n",
      "Time elapsed = 0.0 min and 58.13286089897156s (58.13286089897156s)\n",
      "Convergence after 8 epochs: Cost = 2293892.7629350536, Train MSE = 4.107019500383177\n",
      "#-----\n",
      "Iteration 26/100: Valid MSE = 4.214003608695952, Combo Parameters = (1, 1.3333333333333333, 3.333333333333333)\n",
      "Time elapsed = 0.0 min and 49.57633876800537s (49.57633876800537s)\n",
      "Convergence after 9 epochs: Cost = 2275380.6060548793, Train MSE = 4.069424866402086\n",
      "#-----\n",
      "Iteration 27/100: Valid MSE = 4.165863331561244, Combo Parameters = (1, 1.3333333333333333, 4.0)\n",
      "Time elapsed = 0.0 min and 52.795111417770386s (52.795111417770386s)\n",
      "Convergence after 8 epochs: Cost = 2290878.566845451, Train MSE = 4.091339647412064\n",
      "#-----\n",
      "Iteration 28/100: Valid MSE = 4.183660914884399, Combo Parameters = (1, 1.3333333333333333, 4.666666666666666)\n",
      "Time elapsed = 0.0 min and 48.504340410232544s (48.504340410232544s)\n",
      "Convergence after 9 epochs: Cost = 2285404.059417107, Train MSE = 4.07692467312898\n",
      "#-----\n",
      "Iteration 29/100: Valid MSE = 4.174267708786273, Combo Parameters = (1, 1.3333333333333333, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 53.77284336090088s (53.77284336090088s)\n",
      "Convergence after 7 epochs: Cost = 2312126.2350359885, Train MSE = 4.122776863755736\n",
      "#-----\n",
      "Iteration 30/100: Valid MSE = 4.214104140730344, Combo Parameters = (1, 1.3333333333333333, 6.0)\n",
      "Time elapsed = 0.0 min and 42.881664752960205s (42.881664752960205s)\n",
      "Convergence after 11 epochs: Cost = 2291433.206592829, Train MSE = 4.1438486597676985\n",
      "#-----\n",
      "Iteration 31/100: Valid MSE = 6.069580296431845, Combo Parameters = (1, 2.0, 0.0)\n",
      "Time elapsed = 1.0 min and 7.9365057945251465s (67.93650579452515s)\n",
      "Convergence after 12 epochs: Cost = 2257450.613404136, Train MSE = 4.056358081424797\n",
      "#-----\n",
      "Iteration 32/100: Valid MSE = 4.170258081038456, Combo Parameters = (1, 2.0, 0.6666666666666666)\n",
      "Time elapsed = 1.0 min and 12.993295431137085s (72.99329543113708s)\n",
      "Convergence after 9 epochs: Cost = 2276027.9846861335, Train MSE = 4.076431018086582\n",
      "#-----\n",
      "Iteration 33/100: Valid MSE = 4.181750591070408, Combo Parameters = (1, 2.0, 1.3333333333333333)\n",
      "Time elapsed = 0.0 min and 54.550617694854736s (54.550617694854736s)\n",
      "Convergence after 8 epochs: Cost = 2289559.271748573, Train MSE = 4.091708888902435\n",
      "#-----\n",
      "Iteration 34/100: Valid MSE = 4.201680947909428, Combo Parameters = (1, 2.0, 2.0)\n",
      "Time elapsed = 0.0 min and 49.21840000152588s (49.21840000152588s)\n",
      "Convergence after 11 epochs: Cost = 2278113.9741867417, Train MSE = 4.064851914244054\n",
      "#-----\n",
      "Iteration 35/100: Valid MSE = 4.166797361044998, Combo Parameters = (1, 2.0, 2.6666666666666665)\n",
      "Time elapsed = 1.0 min and 8.545302152633667s (68.54530215263367s)\n",
      "Convergence after 10 epochs: Cost = 2291220.7392538497, Train MSE = 4.083929366182117\n",
      "#-----\n",
      "Iteration 36/100: Valid MSE = 4.184665189389524, Combo Parameters = (1, 2.0, 3.333333333333333)\n",
      "Time elapsed = 1.0 min and 4.57800817489624s (64.57800817489624s)\n",
      "Convergence after 7 epochs: Cost = 2319861.06902763, Train MSE = 4.1300131570911205\n",
      "#-----\n",
      "Iteration 37/100: Valid MSE = 4.239028800267351, Combo Parameters = (1, 2.0, 4.0)\n",
      "Time elapsed = 0.0 min and 45.020002126693726s (45.020002126693726s)\n",
      "Convergence after 10 epochs: Cost = 2306155.7021337147, Train MSE = 4.10028700250494\n",
      "#-----\n",
      "Iteration 38/100: Valid MSE = 4.194499884576275, Combo Parameters = (1, 2.0, 4.666666666666666)\n",
      "Time elapsed = 1.0 min and 9.531068563461304s (69.5310685634613s)\n",
      "Convergence after 8 epochs: Cost = 2320433.7272472368, Train MSE = 4.121557119851641\n",
      "#-----\n",
      "Iteration 39/100: Valid MSE = 4.224001653293487, Combo Parameters = (1, 2.0, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 52.34351062774658s (52.34351062774658s)\n",
      "Convergence after 8 epochs: Cost = 2331051.273251868, Train MSE = 4.138436677942596\n",
      "#-----\n",
      "Iteration 40/100: Valid MSE = 4.239243438380429, Combo Parameters = (1, 2.0, 6.0)\n",
      "Time elapsed = 0.0 min and 50.6152982711792s (50.6152982711792s)\n",
      "Convergence after 11 epochs: Cost = 2306071.7305857176, Train MSE = 4.154784157952377\n",
      "#-----\n",
      "Iteration 41/100: Valid MSE = 7.121657477421384, Combo Parameters = (1, 2.6666666666666665, 0.0)\n",
      "Time elapsed = 1.0 min and 7.983811616897583s (67.98381161689758s)\n",
      "Convergence after 10 epochs: Cost = 2261603.8475249177, Train MSE = 4.046317137507169\n",
      "#-----\n",
      "Iteration 42/100: Valid MSE = 4.164899715541695, Combo Parameters = (1, 2.6666666666666665, 0.6666666666666666)\n",
      "Time elapsed = 1.0 min and 2.633620262145996s (62.633620262145996s)\n",
      "Convergence after 11 epochs: Cost = 2297225.4689767817, Train MSE = 4.099041190033022\n",
      "#-----\n",
      "Iteration 43/100: Valid MSE = 4.205216966639545, Combo Parameters = (1, 2.6666666666666665, 1.3333333333333333)\n",
      "Time elapsed = 1.0 min and 9.476851224899292s (69.47685122489929s)\n",
      "Convergence after 9 epochs: Cost = 2291754.826366075, Train MSE = 4.078180286285839\n",
      "#-----\n",
      "Iteration 44/100: Valid MSE = 4.180822385491555, Combo Parameters = (1, 2.6666666666666665, 2.0)\n",
      "Time elapsed = 0.0 min and 56.22691082954407s (56.22691082954407s)\n",
      "Convergence after 10 epochs: Cost = 2295798.9063964183, Train MSE = 4.079982658580511\n",
      "#-----\n",
      "Iteration 45/100: Valid MSE = 4.177795243580074, Combo Parameters = (1, 2.6666666666666665, 2.6666666666666665)\n",
      "Time elapsed = 1.0 min and 1.1066107749938965s (61.1066107749939s)\n",
      "Convergence after 8 epochs: Cost = 2323617.4584656283, Train MSE = 4.121687739056385\n",
      "#-----\n",
      "Iteration 46/100: Valid MSE = 4.212582805792004, Combo Parameters = (1, 2.6666666666666665, 3.333333333333333)\n",
      "Time elapsed = 0.0 min and 50.85961127281189s (50.85961127281189s)\n",
      "Convergence after 7 epochs: Cost = 2327838.7295645284, Train MSE = 4.124903282436025\n",
      "#-----\n",
      "Iteration 47/100: Valid MSE = 4.214121886755601, Combo Parameters = (1, 2.6666666666666665, 4.0)\n",
      "Time elapsed = 0.0 min and 43.520687103271484s (43.520687103271484s)\n",
      "Convergence after 9 epochs: Cost = 2325196.202808456, Train MSE = 4.118096070256777\n",
      "#-----\n",
      "Iteration 48/100: Valid MSE = 4.21149975246487, Combo Parameters = (1, 2.6666666666666665, 4.666666666666666)\n",
      "Time elapsed = 0.0 min and 56.549408197402954s (56.549408197402954s)\n",
      "Convergence after 9 epochs: Cost = 2317225.6756941346, Train MSE = 4.097645436298898\n",
      "#-----\n",
      "Iteration 49/100: Valid MSE = 4.189511433197832, Combo Parameters = (1, 2.6666666666666665, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 55.84020781517029s (55.84020781517029s)\n",
      "Convergence after 8 epochs: Cost = 2337652.734891974, Train MSE = 4.129949561754316\n",
      "#-----\n",
      "Iteration 50/100: Valid MSE = 4.2225968239444445, Combo Parameters = (1, 2.6666666666666665, 6.0)\n",
      "Time elapsed = 0.0 min and 49.141228914260864s (49.141228914260864s)\n",
      "Convergence after 14 epochs: Cost = 2305440.8740673508, Train MSE = 4.1423447396587605\n",
      "#-----\n",
      "Iteration 51/100: Valid MSE = 357519.84575403435, Combo Parameters = (1, 3.333333333333333, 0.0)\n",
      "Time elapsed = 1.0 min and 25.900842428207397s (85.9008424282074s)\n",
      "Convergence after 13 epochs: Cost = 2275113.2084309054, Train MSE = 4.060404271351042\n",
      "#-----\n",
      "Iteration 52/100: Valid MSE = 4.169738272385115, Combo Parameters = (1, 3.333333333333333, 0.6666666666666666)\n",
      "Time elapsed = 1.0 min and 17.210171222686768s (77.21017122268677s)\n",
      "Convergence after 9 epochs: Cost = 2308770.9427113524, Train MSE = 4.10460568089769\n",
      "#-----\n",
      "Iteration 53/100: Valid MSE = 4.21530475926112, Combo Parameters = (1, 3.333333333333333, 1.3333333333333333)\n",
      "Time elapsed = 0.0 min and 52.81076121330261s (52.81076121330261s)\n",
      "Convergence after 9 epochs: Cost = 2298722.7474238044, Train MSE = 4.077942073537779\n",
      "#-----\n",
      "Iteration 54/100: Valid MSE = 4.180163389130776, Combo Parameters = (1, 3.333333333333333, 2.0)\n",
      "Time elapsed = 0.0 min and 52.49234890937805s (52.49234890937805s)\n",
      "Convergence after 7 epochs: Cost = 2331013.658552477, Train MSE = 4.125161051709452\n",
      "#-----\n",
      "Iteration 55/100: Valid MSE = 4.2331570782405805, Combo Parameters = (1, 3.333333333333333, 2.6666666666666665)\n",
      "Time elapsed = 0.0 min and 41.221211433410645s (41.221211433410645s)\n",
      "Convergence after 7 epochs: Cost = 2332278.407235208, Train MSE = 4.122550462077542\n",
      "#-----\n",
      "Iteration 56/100: Valid MSE = 4.217385328444794, Combo Parameters = (1, 3.333333333333333, 3.333333333333333)\n",
      "Time elapsed = 0.0 min and 42.169095516204834s (42.169095516204834s)\n",
      "Convergence after 9 epochs: Cost = 2327638.089153179, Train MSE = 4.111425651961876\n",
      "#-----\n",
      "Iteration 57/100: Valid MSE = 4.205422997255448, Combo Parameters = (1, 3.333333333333333, 4.0)\n",
      "Time elapsed = 0.0 min and 56.55543279647827s (56.55543279647827s)\n",
      "Convergence after 9 epochs: Cost = 2340050.9673882555, Train MSE = 4.129340983356831\n",
      "#-----\n",
      "Iteration 58/100: Valid MSE = 4.223511420171844, Combo Parameters = (1, 3.333333333333333, 4.666666666666666)\n",
      "Time elapsed = 0.0 min and 56.894999265670776s (56.894999265670776s)\n",
      "Convergence after 9 epochs: Cost = 2332966.6661809557, Train MSE = 4.110695595590471\n",
      "#-----\n",
      "Iteration 59/100: Valid MSE = 4.202183608011856, Combo Parameters = (1, 3.333333333333333, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 57.44122767448425s (57.44122767448425s)\n",
      "Convergence after 10 epochs: Cost = 2345836.003903321, Train MSE = 4.1313604010638585\n",
      "#-----\n",
      "Iteration 60/100: Valid MSE = 4.21656902060562, Combo Parameters = (1, 3.333333333333333, 6.0)\n",
      "Time elapsed = 1.0 min and 3.02101993560791s (63.02101993560791s)\n",
      "Convergence after 23 epochs: Cost = 2303042.8842517594, Train MSE = 4.13163317289665\n",
      "#-----\n",
      "Iteration 61/100: Valid MSE = 626.0803750806624, Combo Parameters = (1, 4.0, 0.0)\n",
      "Time elapsed = 2.0 min and 17.94089365005493s (137.94089365005493s)\n",
      "Convergence after 10 epochs: Cost = 2285203.7254536585, Train MSE = 4.065096611601746\n",
      "#-----\n",
      "Iteration 62/100: Valid MSE = 4.173335503088404, Combo Parameters = (1, 4.0, 0.6666666666666666)\n",
      "Time elapsed = 0.0 min and 59.27920985221863s (59.27920985221863s)\n",
      "Convergence after 10 epochs: Cost = 2302563.3615601165, Train MSE = 4.082609183235669\n",
      "#-----\n",
      "Iteration 63/100: Valid MSE = 4.185703112004277, Combo Parameters = (1, 4.0, 1.3333333333333333)\n",
      "Time elapsed = 1.0 min and 1.0172951221466064s (61.017295122146606s)\n",
      "Convergence after 11 epochs: Cost = 2333935.0854356675, Train MSE = 4.129189888463222\n",
      "#-----\n",
      "Iteration 64/100: Valid MSE = 4.2419690331492905, Combo Parameters = (1, 4.0, 2.0)\n",
      "Time elapsed = 1.0 min and 8.777803659439087s (68.77780365943909s)\n",
      "Convergence after 9 epochs: Cost = 2329006.514766915, Train MSE = 4.112125568357579\n",
      "#-----\n",
      "Iteration 65/100: Valid MSE = 4.204435225388627, Combo Parameters = (1, 4.0, 2.6666666666666665)\n",
      "Time elapsed = 0.0 min and 57.90212106704712s (57.90212106704712s)\n",
      "Convergence after 10 epochs: Cost = 2322410.406406509, Train MSE = 4.095699128496087\n",
      "#-----\n",
      "Iteration 66/100: Valid MSE = 4.196359163380631, Combo Parameters = (1, 4.0, 3.333333333333333)\n",
      "Time elapsed = 1.0 min and 2.4953575134277344s (62.495357513427734s)\n",
      "Convergence after 11 epochs: Cost = 2329882.732612117, Train MSE = 4.103514464836313\n",
      "#-----\n",
      "Iteration 67/100: Valid MSE = 4.19678851492183, Combo Parameters = (1, 4.0, 4.0)\n",
      "Time elapsed = 1.0 min and 10.80793309211731s (70.80793309211731s)\n",
      "Convergence after 9 epochs: Cost = 2334474.0729617607, Train MSE = 4.105166479474863\n",
      "#-----\n",
      "Iteration 68/100: Valid MSE = 4.193080500539236, Combo Parameters = (1, 4.0, 4.666666666666666)\n",
      "Time elapsed = 0.0 min and 58.24542593955994s (58.24542593955994s)\n",
      "Convergence after 13 epochs: Cost = 2340201.804914261, Train MSE = 4.1110285440322745\n",
      "#-----\n",
      "Iteration 69/100: Valid MSE = 4.200243412621956, Combo Parameters = (1, 4.0, 5.333333333333333)\n",
      "Time elapsed = 1.0 min and 20.484156847000122s (80.48415684700012s)\n",
      "Convergence after 10 epochs: Cost = 2343389.4463881543, Train MSE = 4.114282528135185\n",
      "#-----\n",
      "Iteration 70/100: Valid MSE = 4.205805440632549, Combo Parameters = (1, 4.0, 6.0)\n",
      "Time elapsed = 1.0 min and 0.3102092742919922s (60.31020927429199s)\n",
      "Convergence after 10 epochs: Cost = 2345223.256644234, Train MSE = 4.19180709354954\n",
      "#-----\n",
      "Iteration 71/100: Valid MSE = 5.2825126351366345, Combo Parameters = (1, 4.666666666666666, 0.0)\n",
      "Time elapsed = 0.0 min and 57.26886010169983s (57.26886010169983s)\n",
      "Convergence after 11 epochs: Cost = 2314515.483451371, Train MSE = 4.103521194016515\n",
      "#-----\n",
      "Iteration 72/100: Valid MSE = 4.2210078856261255, Combo Parameters = (1, 4.666666666666666, 0.6666666666666666)\n",
      "Time elapsed = 0.0 min and 58.48330116271973s (58.48330116271973s)\n",
      "Convergence after 9 epochs: Cost = 2334211.6509211273, Train MSE = 4.123093486643518\n",
      "#-----\n",
      "Iteration 73/100: Valid MSE = 4.226509624335402, Combo Parameters = (1, 4.666666666666666, 1.3333333333333333)\n",
      "Time elapsed = 0.0 min and 47.57076454162598s (47.57076454162598s)\n",
      "Convergence after 11 epochs: Cost = 2316730.3206498376, Train MSE = 4.090884371806234\n",
      "#-----\n",
      "Iteration 74/100: Valid MSE = 4.19454546468504, Combo Parameters = (1, 4.666666666666666, 2.0)\n",
      "Time elapsed = 1.0 min and 0.6477396488189697s (60.64773964881897s)\n",
      "Convergence after 9 epochs: Cost = 2330662.9651496853, Train MSE = 4.102620542675223\n",
      "#-----\n",
      "Iteration 75/100: Valid MSE = 4.20193399065578, Combo Parameters = (1, 4.666666666666666, 2.6666666666666665)\n",
      "Time elapsed = 0.0 min and 48.32439160346985s (48.32439160346985s)\n",
      "Convergence after 8 epochs: Cost = 2353213.110340016, Train MSE = 4.136388275022472\n",
      "#-----\n",
      "Iteration 76/100: Valid MSE = 4.232877491635265, Combo Parameters = (1, 4.666666666666666, 3.333333333333333)\n",
      "Time elapsed = 0.0 min and 43.02239680290222s (43.02239680290222s)\n",
      "Convergence after 13 epochs: Cost = 2335317.361163694, Train MSE = 4.103711668378313\n",
      "#-----\n",
      "Iteration 77/100: Valid MSE = 4.189639715775118, Combo Parameters = (1, 4.666666666666666, 4.0)\n",
      "Time elapsed = 1.0 min and 9.143451690673828s (69.14345169067383s)\n",
      "Convergence after 7 epochs: Cost = 2371932.328822197, Train MSE = 4.158995244235713\n",
      "#-----\n",
      "Iteration 78/100: Valid MSE = 4.251538981897665, Combo Parameters = (1, 4.666666666666666, 4.666666666666666)\n",
      "Time elapsed = 0.0 min and 36.44960951805115s (36.44960951805115s)\n",
      "Convergence after 9 epochs: Cost = 2369186.253703447, Train MSE = 4.153210034778115\n",
      "#-----\n",
      "Iteration 79/100: Valid MSE = 4.244712491464846, Combo Parameters = (1, 4.666666666666666, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 47.11408758163452s (47.11408758163452s)\n",
      "Convergence after 9 epochs: Cost = 2356851.9299242077, Train MSE = 4.1259762202769545\n",
      "#-----\n",
      "Iteration 80/100: Valid MSE = 4.208663780237198, Combo Parameters = (1, 4.666666666666666, 6.0)\n",
      "Time elapsed = 0.0 min and 47.12561821937561s (47.12561821937561s)\n",
      "Convergence after 14 epochs: Cost = 2363514.257199931, Train MSE = 4.209029721149701\n",
      "#-----\n",
      "Iteration 81/100: Valid MSE = 293.6973709299501, Combo Parameters = (1, 5.333333333333333, 0.0)\n",
      "Time elapsed = 1.0 min and 13.642837524414062s (73.64283752441406s)\n",
      "Convergence after 12 epochs: Cost = 2305686.9748264304, Train MSE = 4.079392746264062\n",
      "#-----\n",
      "Iteration 82/100: Valid MSE = 4.1889640507854935, Combo Parameters = (1, 5.333333333333333, 0.6666666666666666)\n",
      "Time elapsed = 1.0 min and 2.738798141479492s (62.73879814147949s)\n",
      "Convergence after 10 epochs: Cost = 2321257.977690511, Train MSE = 4.093289115861273\n",
      "#-----\n",
      "Iteration 83/100: Valid MSE = 4.196617093901877, Combo Parameters = (1, 5.333333333333333, 1.3333333333333333)\n",
      "Time elapsed = 0.0 min and 52.65080237388611s (52.65080237388611s)\n",
      "Convergence after 12 epochs: Cost = 2325915.7765846835, Train MSE = 4.095099743588557\n",
      "#-----\n",
      "Iteration 84/100: Valid MSE = 4.1911312136480054, Combo Parameters = (1, 5.333333333333333, 2.0)\n",
      "Time elapsed = 1.0 min and 3.46789288520813s (63.46789288520813s)\n",
      "Convergence after 8 epochs: Cost = 2361512.604382145, Train MSE = 4.145986636195196\n",
      "#-----\n",
      "Iteration 85/100: Valid MSE = 4.244546240637293, Combo Parameters = (1, 5.333333333333333, 2.6666666666666665)\n",
      "Time elapsed = 0.0 min and 41.80455684661865s (41.80455684661865s)\n",
      "Convergence after 11 epochs: Cost = 2342965.3427497004, Train MSE = 4.109927429888968\n",
      "#-----\n",
      "Iteration 86/100: Valid MSE = 4.201471018764212, Combo Parameters = (1, 5.333333333333333, 3.333333333333333)\n",
      "Time elapsed = 0.0 min and 58.17969632148743s (58.17969632148743s)\n",
      "Convergence after 11 epochs: Cost = 2362148.4276710106, Train MSE = 4.139521870413051\n",
      "#-----\n",
      "Iteration 87/100: Valid MSE = 4.23112090015893, Combo Parameters = (1, 5.333333333333333, 4.0)\n",
      "Time elapsed = 0.0 min and 58.30022668838501s (58.30022668838501s)\n",
      "Convergence after 9 epochs: Cost = 2356830.5322017944, Train MSE = 4.12255229123073\n",
      "#-----\n",
      "Iteration 88/100: Valid MSE = 4.210248696078892, Combo Parameters = (1, 5.333333333333333, 4.666666666666666)\n",
      "Time elapsed = 0.0 min and 47.03764033317566s (47.03764033317566s)\n",
      "Convergence after 8 epochs: Cost = 2383106.6927505373, Train MSE = 4.1645261410530425\n",
      "#-----\n",
      "Iteration 89/100: Valid MSE = 4.253552992023219, Combo Parameters = (1, 5.333333333333333, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 42.14100193977356s (42.14100193977356s)\n",
      "Convergence after 9 epochs: Cost = 2375238.46809673, Train MSE = 4.14856510757898\n",
      "#-----\n",
      "Iteration 90/100: Valid MSE = 4.230839447727305, Combo Parameters = (1, 5.333333333333333, 6.0)\n",
      "Time elapsed = 0.0 min and 47.15980529785156s (47.15980529785156s)\n",
      "Convergence after 14 epochs: Cost = 2343066.443947055, Train MSE = 4.168458576260677\n",
      "#-----\n",
      "Iteration 91/100: Valid MSE = 70.80436872429527, Combo Parameters = (1, 6.0, 0.0)\n",
      "Time elapsed = 1.0 min and 12.668660879135132s (72.66866087913513s)\n",
      "Convergence after 13 epochs: Cost = 2329080.8273208314, Train MSE = 4.115396861243373\n",
      "#-----\n",
      "Iteration 92/100: Valid MSE = 4.225345271490452, Combo Parameters = (1, 6.0, 0.6666666666666666)\n",
      "Time elapsed = 1.0 min and 8.327675819396973s (68.32767581939697s)\n",
      "Convergence after 11 epochs: Cost = 2327997.0279213917, Train MSE = 4.099858365672901\n",
      "#-----\n",
      "Iteration 93/100: Valid MSE = 4.19904908401956, Combo Parameters = (1, 6.0, 1.3333333333333333)\n",
      "Time elapsed = 0.0 min and 57.417272090911865s (57.417272090911865s)\n",
      "Convergence after 9 epochs: Cost = 2340149.499345968, Train MSE = 4.108357646707873\n",
      "#-----\n",
      "Iteration 94/100: Valid MSE = 4.207369271715692, Combo Parameters = (1, 6.0, 2.0)\n",
      "Time elapsed = 0.0 min and 47.28964138031006s (47.28964138031006s)\n",
      "Convergence after 8 epochs: Cost = 2367516.367105019, Train MSE = 4.14865576707762\n",
      "#-----\n",
      "Iteration 95/100: Valid MSE = 4.2421107371427125, Combo Parameters = (1, 6.0, 2.6666666666666665)\n",
      "Time elapsed = 0.0 min and 41.83296585083008s (41.83296585083008s)\n",
      "Convergence after 11 epochs: Cost = 2349228.8798448076, Train MSE = 4.111060702581656\n",
      "#-----\n",
      "Iteration 96/100: Valid MSE = 4.205869773640947, Combo Parameters = (1, 6.0, 3.333333333333333)\n",
      "Time elapsed = 0.0 min and 57.96729588508606s (57.96729588508606s)\n",
      "Convergence after 8 epochs: Cost = 2381453.8325509573, Train MSE = 4.157971962800462\n",
      "#-----\n",
      "Iteration 97/100: Valid MSE = 4.252849564389117, Combo Parameters = (1, 6.0, 4.0)\n",
      "Time elapsed = 0.0 min and 42.67328691482544s (42.67328691482544s)\n",
      "Convergence after 12 epochs: Cost = 2368677.895669455, Train MSE = 4.134825573358269\n",
      "#-----\n",
      "Iteration 98/100: Valid MSE = 4.23400851807617, Combo Parameters = (1, 6.0, 4.666666666666666)\n",
      "Time elapsed = 1.0 min and 2.8872451782226562s (62.887245178222656s)\n",
      "Convergence after 8 epochs: Cost = 2371967.4590337826, Train MSE = 4.134067897989007\n",
      "#-----\n",
      "Iteration 99/100: Valid MSE = 4.220634907819666, Combo Parameters = (1, 6.0, 5.333333333333333)\n",
      "Time elapsed = 0.0 min and 42.432769775390625s (42.432769775390625s)\n",
      "Convergence after 13 epochs: Cost = 2370970.1069911052, Train MSE = 4.1331918586631335\n",
      "#-----\n",
      "Iteration 100/100: Valid MSE = 4.215241238272321, Combo Parameters = (1, 6.0, 6.0)\n",
      "Time elapsed = 1.0 min and 8.897485971450806s (68.8974859714508s)\n",
      "CPU times: total: 1h 36min 33s\n",
      "Wall time: 1h 37min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Initialize parameters for GRIDSEARCH\n",
    "k_values = [1]\n",
    "lambda_biases = [i for i in np.linspace(0,6,10)]\n",
    "lambda_gammas = [i for i in np.linspace(0,6,10)]\n",
    "n_combos = len(k_values) * len(lambda_biases) * len(lambda_gammas)\n",
    "#\n",
    "ep = 0.005\n",
    "iter_limit = 300\n",
    "output_bounds = (0, np.inf)\n",
    "combos = itertools.product(*(k_values, lambda_biases, lambda_gammas))\n",
    "\n",
    "times = []\n",
    "results = []\n",
    "for i,combo in enumerate(combos):\n",
    "    t_start = time.time()\n",
    "    # print(f\"#-----\\nIteration {i + 1}/{n_combos}\")\n",
    "    k, lambda_bias, lambda_gamma = combo\n",
    "\n",
    "    params = {\n",
    "             \"lambda_bias\": lambda_bias, \"lambda_gamma\": lambda_gamma,\n",
    "             \"k\": k, \"ep\": ep, \"iter_limit\": iter_limit, \"output_bounds\": output_bounds\n",
    "             }\n",
    "    \n",
    "    init_bounds = (-.1,.1)\n",
    "    theta = initialize_params(meanValue, itemsPerUser, usersPerItem, k, init_bounds)\n",
    "    \n",
    "    ### Fit the model to the training data\n",
    "    theta, cost, mse = fit_parameters(X_train.values.tolist(), y_train, theta, quiet=True, **params)\n",
    "    avg_params = get_lfm_defaults(theta)\n",
    "    \n",
    "    ### Predict using the learned parameters\n",
    "    predictions = [predict_latent_factor(u_id, item_id, theta, output_bounds, avg_params) for (u_id,item_id) in X_valid.values.tolist()]\n",
    "    validMSE = get_MSE(predictions, y_valid)\n",
    "    t_elapsed = time.time() - t_start\n",
    "    \n",
    "    # print(f\"Valid MSE = {validMSE}, Combo Parameters = {combo}\")\n",
    "    print(f\"#-----\\nIteration {i + 1}/{n_combos}: Valid MSE = {validMSE}, Combo Parameters = {combo}\")\n",
    "    print(f\"Time elapsed = {t_elapsed // 60} min and {t_elapsed % 60}s ({t_elapsed}s)\")\n",
    "\n",
    "    times.append((t_elapsed, combo))\n",
    "    results.append((validMSE, combo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d1e71e-9491-49cf-aafe-a505f9a2e31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5.421893025540095, (1, 0.0, 0.0)),\n",
       " (4.147657075939483, (1, 0.0, 0.6666666666666666)),\n",
       " (4.181269820582037, (1, 0.0, 1.3333333333333333)),\n",
       " (4.181250347599356, (1, 0.0, 2.0)),\n",
       " (4.159416835482152, (1, 0.0, 2.6666666666666665)),\n",
       " (4.16146237739849, (1, 0.0, 3.333333333333333)),\n",
       " (4.171693298441471, (1, 0.0, 4.0)),\n",
       " (4.192938179754505, (1, 0.0, 4.666666666666666)),\n",
       " (4.199450895728554, (1, 0.0, 5.333333333333333)),\n",
       " (4.1717429834690885, (1, 0.0, 6.0)),\n",
       " (5.256814350883769, (1, 0.6666666666666666, 0.0)),\n",
       " (4.159099081128472, (1, 0.6666666666666666, 0.6666666666666666)),\n",
       " (4.164060133693579, (1, 0.6666666666666666, 1.3333333333333333)),\n",
       " (4.156477008075843, (1, 0.6666666666666666, 2.0)),\n",
       " (4.157950737373503, (1, 0.6666666666666666, 2.6666666666666665)),\n",
       " (4.1607401300464035, (1, 0.6666666666666666, 3.333333333333333)),\n",
       " (4.187252429709311, (1, 0.6666666666666666, 4.0)),\n",
       " (4.191764693522772, (1, 0.6666666666666666, 4.666666666666666)),\n",
       " (4.195936640765638, (1, 0.6666666666666666, 5.333333333333333)),\n",
       " (4.225758995526153, (1, 0.6666666666666666, 6.0)),\n",
       " (7.141127882981046, (1, 1.3333333333333333, 0.0)),\n",
       " (4.19058988624658, (1, 1.3333333333333333, 0.6666666666666666)),\n",
       " (4.182536942733937, (1, 1.3333333333333333, 1.3333333333333333)),\n",
       " (4.155639580505811, (1, 1.3333333333333333, 2.0)),\n",
       " (4.196991162296007, (1, 1.3333333333333333, 2.6666666666666665)),\n",
       " (4.214003608695952, (1, 1.3333333333333333, 3.333333333333333)),\n",
       " (4.165863331561244, (1, 1.3333333333333333, 4.0)),\n",
       " (4.183660914884399, (1, 1.3333333333333333, 4.666666666666666)),\n",
       " (4.174267708786273, (1, 1.3333333333333333, 5.333333333333333)),\n",
       " (4.214104140730344, (1, 1.3333333333333333, 6.0)),\n",
       " (6.069580296431845, (1, 2.0, 0.0)),\n",
       " (4.170258081038456, (1, 2.0, 0.6666666666666666)),\n",
       " (4.181750591070408, (1, 2.0, 1.3333333333333333)),\n",
       " (4.201680947909428, (1, 2.0, 2.0)),\n",
       " (4.166797361044998, (1, 2.0, 2.6666666666666665)),\n",
       " (4.184665189389524, (1, 2.0, 3.333333333333333)),\n",
       " (4.239028800267351, (1, 2.0, 4.0)),\n",
       " (4.194499884576275, (1, 2.0, 4.666666666666666)),\n",
       " (4.224001653293487, (1, 2.0, 5.333333333333333)),\n",
       " (4.239243438380429, (1, 2.0, 6.0)),\n",
       " (7.121657477421384, (1, 2.6666666666666665, 0.0)),\n",
       " (4.164899715541695, (1, 2.6666666666666665, 0.6666666666666666)),\n",
       " (4.205216966639545, (1, 2.6666666666666665, 1.3333333333333333)),\n",
       " (4.180822385491555, (1, 2.6666666666666665, 2.0)),\n",
       " (4.177795243580074, (1, 2.6666666666666665, 2.6666666666666665)),\n",
       " (4.212582805792004, (1, 2.6666666666666665, 3.333333333333333)),\n",
       " (4.214121886755601, (1, 2.6666666666666665, 4.0)),\n",
       " (4.21149975246487, (1, 2.6666666666666665, 4.666666666666666)),\n",
       " (4.189511433197832, (1, 2.6666666666666665, 5.333333333333333)),\n",
       " (4.2225968239444445, (1, 2.6666666666666665, 6.0)),\n",
       " (357519.84575403435, (1, 3.333333333333333, 0.0)),\n",
       " (4.169738272385115, (1, 3.333333333333333, 0.6666666666666666)),\n",
       " (4.21530475926112, (1, 3.333333333333333, 1.3333333333333333)),\n",
       " (4.180163389130776, (1, 3.333333333333333, 2.0)),\n",
       " (4.2331570782405805, (1, 3.333333333333333, 2.6666666666666665)),\n",
       " (4.217385328444794, (1, 3.333333333333333, 3.333333333333333)),\n",
       " (4.205422997255448, (1, 3.333333333333333, 4.0)),\n",
       " (4.223511420171844, (1, 3.333333333333333, 4.666666666666666)),\n",
       " (4.202183608011856, (1, 3.333333333333333, 5.333333333333333)),\n",
       " (4.21656902060562, (1, 3.333333333333333, 6.0)),\n",
       " (626.0803750806624, (1, 4.0, 0.0)),\n",
       " (4.173335503088404, (1, 4.0, 0.6666666666666666)),\n",
       " (4.185703112004277, (1, 4.0, 1.3333333333333333)),\n",
       " (4.2419690331492905, (1, 4.0, 2.0)),\n",
       " (4.204435225388627, (1, 4.0, 2.6666666666666665)),\n",
       " (4.196359163380631, (1, 4.0, 3.333333333333333)),\n",
       " (4.19678851492183, (1, 4.0, 4.0)),\n",
       " (4.193080500539236, (1, 4.0, 4.666666666666666)),\n",
       " (4.200243412621956, (1, 4.0, 5.333333333333333)),\n",
       " (4.205805440632549, (1, 4.0, 6.0)),\n",
       " (5.2825126351366345, (1, 4.666666666666666, 0.0)),\n",
       " (4.2210078856261255, (1, 4.666666666666666, 0.6666666666666666)),\n",
       " (4.226509624335402, (1, 4.666666666666666, 1.3333333333333333)),\n",
       " (4.19454546468504, (1, 4.666666666666666, 2.0)),\n",
       " (4.20193399065578, (1, 4.666666666666666, 2.6666666666666665)),\n",
       " (4.232877491635265, (1, 4.666666666666666, 3.333333333333333)),\n",
       " (4.189639715775118, (1, 4.666666666666666, 4.0)),\n",
       " (4.251538981897665, (1, 4.666666666666666, 4.666666666666666)),\n",
       " (4.244712491464846, (1, 4.666666666666666, 5.333333333333333)),\n",
       " (4.208663780237198, (1, 4.666666666666666, 6.0)),\n",
       " (293.6973709299501, (1, 5.333333333333333, 0.0)),\n",
       " (4.1889640507854935, (1, 5.333333333333333, 0.6666666666666666)),\n",
       " (4.196617093901877, (1, 5.333333333333333, 1.3333333333333333)),\n",
       " (4.1911312136480054, (1, 5.333333333333333, 2.0)),\n",
       " (4.244546240637293, (1, 5.333333333333333, 2.6666666666666665)),\n",
       " (4.201471018764212, (1, 5.333333333333333, 3.333333333333333)),\n",
       " (4.23112090015893, (1, 5.333333333333333, 4.0)),\n",
       " (4.210248696078892, (1, 5.333333333333333, 4.666666666666666)),\n",
       " (4.253552992023219, (1, 5.333333333333333, 5.333333333333333)),\n",
       " (4.230839447727305, (1, 5.333333333333333, 6.0)),\n",
       " (70.80436872429527, (1, 6.0, 0.0)),\n",
       " (4.225345271490452, (1, 6.0, 0.6666666666666666)),\n",
       " (4.19904908401956, (1, 6.0, 1.3333333333333333)),\n",
       " (4.207369271715692, (1, 6.0, 2.0)),\n",
       " (4.2421107371427125, (1, 6.0, 2.6666666666666665)),\n",
       " (4.205869773640947, (1, 6.0, 3.333333333333333)),\n",
       " (4.252849564389117, (1, 6.0, 4.0)),\n",
       " (4.23400851807617, (1, 6.0, 4.666666666666666)),\n",
       " (4.220634907819666, (1, 6.0, 5.333333333333333)),\n",
       " (4.215241238272321, (1, 6.0, 6.0))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "548ce08f-4d8d-41ee-99e5-d8ad8e850ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_mse = 4.275\n",
    "###\n",
    "mse, lambda_b, lambda_g = [], [], []\n",
    "for result in results:\n",
    "    mse.append(result[0])\n",
    "    lambda_b.append(result[1][1])\n",
    "    lambda_g.append(result[1][2])\n",
    "\n",
    "mse = np.array(mse).reshape((10,10)).T\n",
    "mse[np.where(mse > max_mse)] = max_mse\n",
    "# print(mse.shape)\n",
    "\n",
    "plt.imshow(mse, origin=\"lower\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Validation MSE\")\n",
    "plt.xlabel(\"Lambda Bias\")\n",
    "plt.ylabel(\"Lambda Gamma\")\n",
    "plt.savefig(\"images/models_LFM\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f61b16f-31c5-4967-a2e2-dd78dab98257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_lfm_results(results, max_results = 4.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d32d5ee9-918d-40f3-8645-e0e3c7456a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.147657075939483, (1, 0.0, 0.6666666666666666)),\n",
       " (4.155639580505811, (1, 1.3333333333333333, 2.0)),\n",
       " (4.156477008075843, (1, 0.6666666666666666, 2.0)),\n",
       " (4.157950737373503, (1, 0.6666666666666666, 2.6666666666666665)),\n",
       " (4.159099081128472, (1, 0.6666666666666666, 0.6666666666666666))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # GET THE BEST RESULTS\n",
    "results2 = results.copy()\n",
    "results2.sort()\n",
    "\n",
    "results2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea711e2-2630-41a4-a161-01a83d0aac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14551\n",
      "9214\n",
      "568502\n",
      "14551\n",
      "9214\n",
      "3.309611240257363\n"
     ]
    }
   ],
   "source": [
    "##### itemsPerUser\n",
    "itemsPerUser, usersPerItem, valueDict, userAverages, itemAverages, meanValue = pd_get_rec_structs(train_df2, \"user_id\", \"item_id\", class_col)\n",
    "\n",
    "print(len(itemsPerUser))\n",
    "print(len(usersPerItem))\n",
    "print(len(valueDict))\n",
    "print(len(userAverages))\n",
    "print(len(itemAverages))\n",
    "print(meanValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac7ed92-7aeb-414a-a6d1-ccf4cabba838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting parameters...\n",
      "-----\n",
      "-----\n",
      "Epoch 1: Cost = 3195396.6524597583, Train MSE = 4.694866639090616, Time Elapsed: 6.426999092102051\n",
      "-----\n",
      "-----\n",
      "Epoch 2: Cost = 2921109.1552001373, Train MSE = 4.286856317997196, Time Elapsed: 6.616000652313232\n",
      "-----\n",
      "-----\n",
      "Epoch 3: Cost = 2882512.302009994, Train MSE = 4.229157003845997, Time Elapsed: 6.605000257492065\n",
      "-----\n",
      "-----\n",
      "Epoch 4: Cost = 2865821.5531811933, Train MSE = 4.204502423379931, Time Elapsed: 6.792001247406006\n",
      "-----\n",
      "-----\n",
      "Epoch 5: Cost = 2855023.644466845, Train MSE = 4.188766421420933, Time Elapsed: 6.466999530792236\n",
      "-----\n",
      "-----\n",
      "Epoch 6: Cost = 2846754.561366366, Train MSE = 4.176901137288, Time Elapsed: 6.418367624282837\n",
      "-----\n",
      "-----\n",
      "Epoch 7: Cost = 2839226.6302478854, Train MSE = 4.166224261041793, Time Elapsed: 6.067369222640991\n",
      "-----\n",
      "-----\n",
      "Epoch 8: Cost = 2830831.0370678543, Train MSE = 4.154328743787367, Time Elapsed: 6.375384330749512\n",
      "-----\n",
      "-----\n",
      "Epoch 9: Cost = 2820086.4376153303, Train MSE = 4.138985207361931, Time Elapsed: 6.346988916397095\n",
      "-----\n",
      "-----\n",
      "Epoch 10: Cost = 2804657.1921192585, Train MSE = 4.116307054387721, Time Elapsed: 6.3804075717926025\n",
      "-----\n",
      "-----\n",
      "Epoch 11: Cost = 2784245.033180855, Train MSE = 4.085437955821226, Time Elapsed: 6.340911626815796\n",
      "-----\n",
      "-----\n",
      "Epoch 12: Cost = 2767683.547663946, Train MSE = 4.06092984678103, Time Elapsed: 6.312737464904785\n",
      "-----\n",
      "-----\n",
      "Epoch 13: Cost = 2757191.6393509423, Train MSE = 4.045824854842106, Time Elapsed: 6.2970006465911865\n",
      "-----\n",
      "-----\n",
      "Epoch 14: Cost = 2749965.57557699, Train MSE = 4.0356167874223745, Time Elapsed: 6.2799999713897705\n",
      "-----\n",
      "-----\n",
      "Epoch 15: Cost = 2744344.6382664074, Train MSE = 4.027727943347983, Time Elapsed: 6.076993465423584\n",
      "-----\n",
      "-----\n",
      "Epoch 16: Cost = 2739840.815359001, Train MSE = 4.021446169905951, Time Elapsed: 6.439735412597656\n",
      "-----\n",
      "-----\n",
      "Epoch 17: Cost = 2736154.399530934, Train MSE = 4.016357099830344, Time Elapsed: 6.629354953765869\n",
      "-----\n",
      "-----\n",
      "Epoch 18: Cost = 2733155.540812098, Train MSE = 4.012264808512714, Time Elapsed: 6.231770038604736\n",
      "-----\n",
      "-----\n",
      "Epoch 19: Cost = 2730722.2353176675, Train MSE = 4.008959820494337, Time Elapsed: 6.2439799308776855\n",
      "-----\n",
      "-----\n",
      "Epoch 20: Cost = 2728764.320734184, Train MSE = 4.006322548259271, Time Elapsed: 6.528372764587402\n",
      "-----\n",
      "-----\n",
      "Epoch 21: Cost = 2727178.4638159284, Train MSE = 4.004208349197399, Time Elapsed: 6.429421901702881\n",
      "-----\n",
      "-----\n",
      "Epoch 22: Cost = 2725923.4256302454, Train MSE = 4.002561602337713, Time Elapsed: 6.420417547225952\n",
      "-----\n",
      "-----\n",
      "Epoch 23: Cost = 2724903.097958241, Train MSE = 4.001240663863146, Time Elapsed: 6.743000030517578\n",
      "-----\n",
      "-----\n",
      "Epoch 24: Cost = 2724046.3135242118, Train MSE = 4.000137751416394, Time Elapsed: 6.504474878311157\n",
      "-----\n",
      "-----\n",
      "Epoch 25: Cost = 2723317.456920903, Train MSE = 3.999196942285606, Time Elapsed: 6.688464879989624\n",
      "-----\n",
      "-----\n",
      "Epoch 26: Cost = 2722693.617568107, Train MSE = 3.9983947824632105, Time Elapsed: 6.570433855056763\n",
      "-----\n",
      "-----\n",
      "Epoch 27: Cost = 2722144.4456804544, Train MSE = 3.9976891399851797, Time Elapsed: 6.526792526245117\n",
      "-----\n",
      "-----\n",
      "Epoch 28: Cost = 2721654.282358569, Train MSE = 3.997058158480383, Time Elapsed: 6.749297857284546\n",
      "-----\n",
      "-----\n",
      "Epoch 29: Cost = 2721214.517896508, Train MSE = 3.9964904862989603, Time Elapsed: 6.54642391204834\n",
      "-----\n",
      "-----\n",
      "Epoch 30: Cost = 2720818.902916153, Train MSE = 3.9959768369208857, Time Elapsed: 6.632378101348877\n",
      "-----\n",
      "-----\n",
      "Epoch 31: Cost = 2720465.4919042885, Train MSE = 3.995514985970465, Time Elapsed: 6.4440014362335205\n",
      "-----\n",
      "Convergence after 31 epochs: Cost = 2720465.4919042885, Train MSE = 3.995514985970465\n",
      "Training Time elapsed = 200.28150534629822\n",
      "\n",
      "Test MSE = 4.256514015903563\n",
      "Time elapsed = 0.0 min and 0.3300018310546875s (0.3300018310546875s)\n",
      "CPU times: total: 3min 20s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Initialize parameters for \n",
    "k = results2[0][1][0]\n",
    "lambda_bias = results2[0][1][1]\n",
    "lambda_gamma = results2[0][1][2]\n",
    "# k = 1\n",
    "# lambda_bias = 1\n",
    "# lambda_gamma = 1\n",
    "\n",
    "ep = 0.0005\n",
    "iter_limit = 300\n",
    "output_bounds = (0, np.inf)\n",
    "params = {\n",
    "         \"lambda_bias\": lambda_bias, \"lambda_gamma\": lambda_gamma,\n",
    "         \"k\": k, \"ep\": ep, \"iter_limit\": iter_limit, \"output_bounds\": output_bounds\n",
    "         }\n",
    "\n",
    "init_bounds = (-1, 1)\n",
    "theta = initialize_params(meanValue, itemsPerUser, usersPerItem, k, init_bounds)\n",
    "\n",
    "### Fit the model to the training data\n",
    "t_start = time.time()\n",
    "theta, cost, mse = fit_parameters(X_train2.values.tolist(), y_train2, theta, mini_batch=False, n_mini_batch=10, quiet=False, **params)\n",
    "# avg_params = get_lfm_defaults(theta)\n",
    "\n",
    "print(f\"Training Time elapsed = {time.time() - t_start}\\n\")\n",
    "t_start = time.time()\n",
    "\n",
    "### Predict using the learned parameters\n",
    "predictions1 = [predict_latent_factor(u_id, item_id, theta, output_bounds) for (u_id,item_id) in X_test.values.tolist()]\n",
    "testMSE = get_MSE(predictions1, y_test)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Test MSE = {testMSE}\")\n",
    "print(f\"Time elapsed = {t_elapsed // 60} min and {t_elapsed % 60}s ({t_elapsed}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40174153-8101-4802-9c2e-32935862b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>playtime</th>\n",
       "      <th>playtime_log</th>\n",
       "      <th>LFM_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u5084</td>\n",
       "      <td>31130</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.896817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u11861</td>\n",
       "      <td>233270</td>\n",
       "      <td>715</td>\n",
       "      <td>6.573680</td>\n",
       "      <td>3.887373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u4949</td>\n",
       "      <td>220200</td>\n",
       "      <td>232</td>\n",
       "      <td>5.451038</td>\n",
       "      <td>6.423125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u1808</td>\n",
       "      <td>4000</td>\n",
       "      <td>231</td>\n",
       "      <td>5.446737</td>\n",
       "      <td>7.733643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u5401</td>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.275084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169222</th>\n",
       "      <td>u11442</td>\n",
       "      <td>238960</td>\n",
       "      <td>20</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>3.165356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169223</th>\n",
       "      <td>u12710</td>\n",
       "      <td>4000</td>\n",
       "      <td>12936</td>\n",
       "      <td>9.467847</td>\n",
       "      <td>8.055044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169224</th>\n",
       "      <td>u10496</td>\n",
       "      <td>8190</td>\n",
       "      <td>207</td>\n",
       "      <td>5.337538</td>\n",
       "      <td>4.865276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169225</th>\n",
       "      <td>u6112</td>\n",
       "      <td>48700</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.361188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169226</th>\n",
       "      <td>u1492</td>\n",
       "      <td>39000</td>\n",
       "      <td>60</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>4.354250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169227 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  playtime  playtime_log  LFM_preds\n",
       "0        u5084    31130         0      0.000000   0.896817\n",
       "1       u11861   233270       715      6.573680   3.887373\n",
       "2        u4949   220200       232      5.451038   6.423125\n",
       "3        u1808     4000       231      5.446737   7.733643\n",
       "4        u5401     1280         0      0.000000   1.275084\n",
       "...        ...      ...       ...           ...        ...\n",
       "169222  u11442   238960        20      3.044522   3.165356\n",
       "169223  u12710     4000     12936      9.467847   8.055044\n",
       "169224  u10496     8190       207      5.337538   4.865276\n",
       "169225   u6112    48700         0      0.000000   6.361188\n",
       "169226   u1492    39000        60      4.110874   4.354250\n",
       "\n",
       "[169227 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Use the best results on the test set\n",
    "test_results_LFM = test_df[[\"user_id\", \"item_id\", \"playtime\", \"playtime_log\"]].copy()\n",
    "test_results_LFM[\"LFM_preds\"] = predictions1\n",
    "\n",
    "test_results_LFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d797aaeb-9aa1-42cc-96e1-aa6aead6333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the one with the best results\n",
    "\n",
    "test_results_LFM.to_csv(f\"data/test_results_LFM_k{k}_lb{lambda_bias}_lg{lambda_gamma}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08355e3c-17a8-4a5c-bbde-a4945439a677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
