{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4de33b3-b307-469a-b4b3-68de6838659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import time\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528eb567-f457-418b-9c2b-9830f82ae52b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### EVALUATION / METRICS\n",
    "######################################\n",
    "def convert_to_np_array(A):\n",
    "    \"\"\"\n",
    "    If A is not already an array, convert it to an array\n",
    "    \"\"\"\n",
    "    if not isinstance(A, np.ndarray): return np.array(A)\n",
    "    else: return A\n",
    "\n",
    "def get_MSE(A, B):\n",
    "    \"\"\"\n",
    "    Given list A and list B:\n",
    "    Return the mean squared error between A and B\n",
    "    \"\"\"\n",
    "    return np.mean((convert_to_np_array(A) - convert_to_np_array(B))**2)\n",
    "\n",
    "def inner(A, B):\n",
    "    \"\"\"\n",
    "    Return the dot product between list A and list B\n",
    "    \"\"\"\n",
    "    return np.dot(convert_to_np_array(A), convert_to_np_array(B))\n",
    "\n",
    "def get_SSE(A, B):\n",
    "    \"\"\"\n",
    "    Given list A and list B:\n",
    "    Return the sum of squared errors between A and B\n",
    "    \"\"\"\n",
    "    return np.sum((convert_to_np_array(A) - convert_to_np_array(B))**2)\n",
    "\n",
    "def get_SE(A,B):\n",
    "    \"\"\"\n",
    "    Given list A and list B:\n",
    "    Return the squared error between each element\n",
    "    \"\"\"\n",
    "    return (convert_to_np_array(A) - convert_to_np_array(B))**2\n",
    "\n",
    "def get_accuracy(A,B):\n",
    "    \"\"\"\n",
    "    Given list A and list B:\n",
    "    Return the accuracy\n",
    "    \"\"\"\n",
    "    return np.sum(convert_to_np_array(A) == convert_to_np_array(B)) / len(A)\n",
    "\n",
    "def get_BER(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    \"Return the balanced error rate between positive (1) and negative(0) instances\n",
    "    \"\"\"\n",
    "\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    n_pos, n_neg = 0, 0\n",
    "    for actual, pred in zip(y_actual, y_predicted):\n",
    "        if actual==1:\n",
    "            n_pos += 1\n",
    "            if actual==pred:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            n_neg += 1\n",
    "            if actual==pred:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (FN + TP)\n",
    "        \n",
    "    return (1/2) * (FPR + FNR)\n",
    "\n",
    "def get_errorMetrics_binary(y_actual, y_predicted, beta=1):\n",
    "    \"\"\"\n",
    "    Return a set of error metrics between positive (1) and negative (0) instances\n",
    "    This is valid for a binary class case\n",
    "    Return a dictionary containing all calculated values\n",
    "    \"\"\"\n",
    "\n",
    "    output = {}\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    n_pos, n_neg = 0, 0\n",
    "    for actual, pred in zip(y_actual, y_predicted):\n",
    "        if actual==1:\n",
    "            n_pos += 1\n",
    "            if actual==pred:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            n_neg += 1\n",
    "            if actual==pred:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    ###\n",
    "    TPR, FNR = TP / n_pos, FN / n_pos\n",
    "    FPR, TNR = FP / n_neg, TN / n_neg\n",
    "    prec = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    output[\"TP\"], output[\"FP\"], output[\"TN\"], output[\"FN\"] = TP, FP, TN, FN\n",
    "    output[\"TPR\"], output[\"FPR\"], output[\"FNR\"], output[\"TNR\"] = TPR, FPR, FNR, TNR\n",
    "    output[\"precision\"], output[\"recall\"] = prec, recall\n",
    "    output[\"BER\"] = (1/2) * (FPR + FNR)\n",
    "    output[f\"F{beta}_Score\"] = (1 + beta**2) * (prec * recall) / ((beta**2)*prec + recall)\n",
    "    output[\"F_Score\"] = 2 * (prec * recall) / (prec + recall)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3256a23-ad04-49c4-bd33-39837f5e6cdf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### FACTORIZATION MACHINE\n",
    "#\n",
    "#\n",
    "\n",
    "def initialize_weighted_params(user_weights, item_weights, k, init_bounds):\n",
    "    \"\"\"\n",
    "    Return the initialized user/item bias/gamma parameters\n",
    "    --> Add a secondary term to multiply values from the data to each bias\n",
    "    \"\"\"\n",
    "    lower, upper = init_bounds\n",
    "    user_bias, item_bias, user_gamma, item_gamma = {}, {}, {}, {}\n",
    "    #\n",
    "    weighted_users, weighted_items = [], []\n",
    "    user_count, item_count = len(user_weights), len(item_weights)\n",
    "    if user_count > 0: \n",
    "        weighted_users = [(key,val) for key,val in user_weights.items()]\n",
    "        for u_id, weight in weighted_users:\n",
    "            user_bias[u_id] = (random.uniform(lower,upper), weight)\n",
    "            user_gamma[u_id] = (np.array([random.uniform(lower,upper) for ki in range(k)]), weight)\n",
    "    if item_count > 0: \n",
    "        weighted_items = [(key,val) for key,val in item_weights.items()]\n",
    "        for item_id, weight in weighted_items:\n",
    "            item_bias[item_id] = (random.uniform(lower, upper), weight)\n",
    "            item_gamma[item_id] = (np.array([random.uniform(lower,upper) for ki in range(k)]), weight)\n",
    "\n",
    "    return (user_bias, item_bias, user_gamma, item_gamma)\n",
    "\n",
    "def get_id_weights(df, id_col, weight_col=None, weight_type=None, only_nonzero=False):\n",
    "    \"\"\"\n",
    "    Return the weights for id_col based on stats\n",
    "    \"\"\"\n",
    "    if weight_type==1:\n",
    "        # Return 1 as the weight for all ids in id_col\n",
    "        return {key:1 for key in df[id_col].unique()}\n",
    "    else:\n",
    "        if only_nonzero:\n",
    "            df = df[df[weight_col] != 0][[id_col, weight_col]]\n",
    "        return {key:val for key,val in zip(df[id_col], df[weight_col])}\n",
    "\n",
    "def predict_latent_factor_batch(Xtrain, theta, k, value_bounds=None, avgParams=None, training=None):\n",
    "    \"\"\"\n",
    "    Return the prediction based on u_id and item_id\n",
    "    If u_id is not in itemsPerUser --> Use the average gamma vector\n",
    "    Repeat this process for if item_id is not in usersPerItem\n",
    "\n",
    "    Bound the output by the min and max of the possible values\n",
    "    (ex: Model shouldn't exceed 5 when the scale is 5)\n",
    "    \"\"\"\n",
    "    n_rows = len(Xtrain)\n",
    "    if value_bounds is None: value_bounds = (-np.inf, np.inf)\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "    #\n",
    "    bias1, gamma1 = user_bias, user_gamma\n",
    "    bias2, gamma2 = item_bias, item_gamma\n",
    "    gbias1, ggamma1 = guser_bias, guser_gamma\n",
    "    gbias2, ggamma2 = gitem_bias, gitem_gamma\n",
    "    #\n",
    "    alphas = np.full(n_rows, alpha)\n",
    "    biases = np.zeros((n_rows, (1 + len(g_keys))*2))\n",
    "    gammas = np.zeros((n_rows, (1 + len(g_keys))*2, k))\n",
    "    for i,tuple in enumerate(Xtrain):\n",
    "        u_id, item_id = tuple[0], tuple[1]\n",
    "        #\n",
    "        biases1 = [bias1] + [gbias1[g_key] if g_key in gbias1 else {} for g_key in g_keys]\n",
    "        gammas1 = [gamma1] + [ggamma1[g_key] if g_key in ggamma1 else {} for g_key in g_keys]\n",
    "        biases2 = [bias2] + [gbias2[g_key] if g_key in gbias2 else {} for g_key in g_keys]\n",
    "        gammas2 = [gamma2] + [ggamma2[g_key] if g_key in ggamma2 else {} for g_key in g_keys]\n",
    "        #\n",
    "        biases[i,:] = get_biases(u_id, biases1) + get_biases(item_id, biases2)\n",
    "        gammas[i,:,:] = get_gammas(u_id, k, gammas1) + get_gammas(item_id, k, gammas2)\n",
    "    C2 = np.square(np.sum(gammas, axis=1))\n",
    "    D  = np.sum(np.square(gammas), axis=1)\n",
    "    gamma_term = (1/2) * (C2 - D).sum(axis=1)\n",
    "    bias_term = biases.sum(axis=1)\n",
    "    predictions = alphas + bias_term + gamma_term\n",
    "\n",
    "    # Adjust for value_bounds:\n",
    "    predictions[predictions < value_bounds[0]] = value_bounds[0]\n",
    "    predictions[predictions > value_bounds[1]] = value_bounds[1]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def get_cost_mse(theta, lambdas, Xtrain, ytrain, k, value_bounds=None):\n",
    "    \"\"\"\n",
    "    Calculate the cost for the given theta parameters\n",
    "    Xtrain must be of form [(user, item), (user, item), ...]\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "    lambda_user_bias, lambda_user_gamma = lambdas[\"lambda_user_bias\"], lambdas[\"lambda_user_gamma\"]\n",
    "    lambda_item_bias, lambda_item_gamma = lambdas[\"lambda_item_bias\"], lambdas[\"lambda_item_gamma\"]\n",
    "    # Predict using the current theta values\n",
    "    predictions = predict_latent_factor_batch(Xtrain, theta, k, training=True)\n",
    "    # Get training mse\n",
    "    mse = get_MSE(predictions, ytrain)\n",
    "    # Calculate SSE + regularization\n",
    "    cost = get_SSE(predictions, ytrain)\n",
    "    cost += lambda_user_bias * np.sum(np.array([val[0]**2 for val in user_bias.values()]))\n",
    "    cost += lambda_item_bias * np.sum(np.array([val[0]**2 for val in item_bias.values()]))\n",
    "    cost += lambda_user_gamma * np.sum(np.array([np.dot(gam[0], gam[0]) for gam in user_gamma.values()]))\n",
    "    cost += lambda_item_gamma * np.sum(np.array([np.dot(gam[0], gam[0]) for gam in item_gamma.values()]))\n",
    "    #\n",
    "    for g_key in g_keys:\n",
    "        cost += lambda_user_bias * np.sum(np.array([val[0]**2 for val in guser_bias[g_key].values()]))\n",
    "        cost += lambda_item_bias * np.sum(np.array([val[0]**2 for val in gitem_bias[g_key].values()]))\n",
    "        cost += lambda_user_gamma * np.sum(np.array([np.dot(gam[0], gam[0]) for gam in guser_gamma[g_key].values()]))\n",
    "        cost += lambda_item_gamma * np.sum(np.array([np.dot(gam[0], gam[0]) for gam in gitem_gamma[g_key].values()]))\n",
    "\n",
    "    return (cost, mse)\n",
    "\n",
    "def get_biases(id, bias_terms):\n",
    "    return [bias_term[id][0] * bias_term[id][1] if id in bias_term else 0 for bias_term in bias_terms]\n",
    "\n",
    "def get_gammas(id, k, gamma_terms):\n",
    "    return [gamma_term[id][0] * gamma_term[id][1] if id in gamma_term else np.zeros(k) for gamma_term in gamma_terms]\n",
    "\n",
    "def get_gterms(gterms, g_keys):\n",
    "    return [gterms[g_key] if g_key in gterms else {} for g_key in g_keys]\n",
    "\n",
    "def get_batches(x, n):\n",
    "    limit = len(x)\n",
    "    for i in range(0, limit, n):\n",
    "        yield x[i:min(i + n, limit)]\n",
    "\n",
    "def fit_parameters(Xtrain, ytrain, theta, ep=0.005, iter_limit=200, quiet=True, quiet2=True, value_bounds=(-np.inf, np.inf), mini_batch=False, n_mini_batch=None, step=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Fit the parameters until convergence (when difference in cost is less than ep)\n",
    "    Arguments packed into **kwargs:\n",
    "    lambda_bias --> Regularization parameter for the user/item biases\n",
    "    lambda_gamma --> Regularization parameter for the user/item gamma matrix\n",
    "    k --> Number of latent parameters to use per user/item vector\n",
    "    ep --> The threshold for early stopping between mse checks\n",
    "    iter_limit --> The maximum number of iterations allowed (epochs)\n",
    "    value_bounds --> The expected range of values expected to be outputted by the model\n",
    "    \"\"\"\n",
    "    t_fit = time.time()\n",
    "    last_mse, last_cost = np.inf, np.inf\n",
    "    best_params = [theta, last_mse, last_cost]\n",
    "    epoch_count = 0\n",
    "    if mini_batch:\n",
    "        if n_mini_batch is None: n_mini_batch = 5\n",
    "        batch_size = math.ceil(len(ytrain) / n_mini_batch)\n",
    "\n",
    "    if not quiet:\n",
    "        print(\"Fitting parameters...\\n-----\")\n",
    "    while True:\n",
    "        epoch_count += 1\n",
    "        ### Update theta\n",
    "        t_theta = time.time()\n",
    "        if mini_batch:\n",
    "            X_y = list(zip(Xtrain, ytrain))\n",
    "            random.shuffle(X_y)\n",
    "            Xtrain, ytrain = zip(*X_y)\n",
    "            for i,(X_batch, y_batch) in enumerate(zip(get_batches(Xtrain, batch_size), get_batches(ytrain, batch_size))):\n",
    "                print(f\"Batch {i + 1}/{n_mini_batch}\")\n",
    "                theta = update_params(theta, lambdas, X_batch, y_batch, k)\n",
    "        else:\n",
    "            # theta = update_params_hybrid(theta, lambdas, Xtrain, ytrain, k)\n",
    "            theta = update_params_coordinateDescent(theta, lambdas, Xtrain, ytrain, k, quiet2=quiet2)\n",
    "            # theta = update_params_blockCoordinateDescent(theta, lambdas, Xtrain, ytrain, k)\n",
    "            # theta = update_params_gradientDescent(theta, lambdas, Xtrain, ytrain, k, step=step)\n",
    "\n",
    "        cost, mse = get_cost_mse(theta, lambdas, Xtrain, ytrain, k, value_bounds)\n",
    "        if not quiet:\n",
    "            print(f\"-----> Epoch {epoch_count}: Cost = {cost}, Train MSE = {mse}, Time Elapsed = {time.time() - t_theta}\")\n",
    "        ### Save current params as best_params if the mse is less than the last mse\n",
    "        if mse < best_params[1]:\n",
    "            best_params = [theta, mse, cost]\n",
    "        ### Check if cost is too high - sometimes the algorithm diverges\n",
    "        if mse > 500000:\n",
    "            theta, mse, cost = best_params\n",
    "            print(f\"Training MSE too high: Best Train MSE = {best_params[1]}, Total Time Elapsed = {time.time() - t_fit}\")\n",
    "            break\n",
    "        ### Early stop if the ep condition is met\n",
    "        if (abs(last_mse - mse) > ep):\n",
    "            last_mse = mse\n",
    "            last_cost = cost\n",
    "        else:\n",
    "            print(f\"Convergence after {epoch_count} epochs: Cost = {cost}, Train MSE = {mse}, Total Time Elapsed = {time.time() - t_fit}\")\n",
    "            break\n",
    "        ### If the iteration limit is reached, stop and return the best parameters\n",
    "        if epoch_count > iter_limit:\n",
    "            theta, mse, cost = best_params\n",
    "            print(f\"Iteration limit reached after {epoch_count} epochs: Best Train MSE = {best_params[1]}, Total Time Elapsed = {time.time() - t_fit}\")\n",
    "            break\n",
    "\n",
    "    return (theta, cost, mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f92726-4534-4425-899d-b66945a9e5c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Coordinate Descent for FM --> ALPHA\n",
    "def update_alpha(theta, lambdas, Xtrain, ytrain, k):\n",
    "    #\n",
    "    lambda_user_bias, lambda_user_gamma = lambdas[\"lambda_user_bias\"], lambdas[\"lambda_user_gamma\"]\n",
    "    lambda_item_bias, lambda_item_gamma = lambdas[\"lambda_item_bias\"], lambdas[\"lambda_item_gamma\"]\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "    #\n",
    "    bias1, gamma1 = user_bias, user_gamma\n",
    "    bias2, gamma2 = item_bias, item_gamma\n",
    "    gbias1, ggamma1 = guser_bias, guser_gamma\n",
    "    gbias2, ggamma2 = gitem_bias, gitem_gamma\n",
    "    #\n",
    "    n_rows = len(y_train)\n",
    "    ratings = np.array(y_train)\n",
    "    biases = np.zeros((n_rows, (1 + len(g_keys))*2))\n",
    "    gammas = np.zeros((n_rows, (1 + len(g_keys))*2, k))\n",
    "    #\n",
    "    for i,((u_id, item_id), rating) in enumerate(zip(Xtrain, ytrain)):\n",
    "        # Extract values\n",
    "        biases1 = [bias1] + [gbias1[g_key] if g_key in gbias1 else {} for g_key in g_keys]\n",
    "        gammas1 = [gamma1] + [ggamma1[g_key] if g_key in ggamma1 else {} for g_key in g_keys]\n",
    "        biases2 = [bias2] + [gbias2[g_key] if g_key in gbias2 else {} for g_key in g_keys]\n",
    "        gammas2 = [gamma2] + [ggamma2[g_key] if g_key in ggamma2 else {} for g_key in g_keys]\n",
    "        biases[i,:] = get_biases(u_id, biases1) + get_biases(item_id, biases2)\n",
    "        gammas[i,:,:] = get_gammas(u_id, k, gammas1) + get_gammas(item_id, k, gammas2)\n",
    "    C2 = np.square(np.sum(gammas, axis=1))\n",
    "    D  = np.sum(np.square(gammas), axis=1)\n",
    "    gamma_term = (1/2) * (C2 - D).sum(axis=1)\n",
    "    bias_term = biases.sum(axis=1)\n",
    "    alpha = (ratings - bias_term - gamma_term).sum() / n_rows\n",
    "    return alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e31d676b-c851-4c2a-8e01-189fd21e989d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Coordinate Descent for FM --> BIASES\n",
    "def update_biases(theta, lambdas, k, update_type):\n",
    "    \"\"\"\n",
    "    Update bias1 to dbias1\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "    lambda_user_bias, lambda_user_gamma = lambdas[\"lambda_user_bias\"], lambdas[\"lambda_user_gamma\"]\n",
    "    lambda_item_bias, lambda_item_gamma = lambdas[\"lambda_item_bias\"], lambdas[\"lambda_item_gamma\"]\n",
    "    if update_type == \"user\":\n",
    "        dict1, bias1, gamma1 = itemsPerUser, user_bias, user_gamma\n",
    "        dict2, bias2, gamma2 = usersPerItem, item_bias, item_gamma\n",
    "        gbias1, ggamma1 = guser_bias, guser_gamma\n",
    "        gbias2, ggamma2 = gitem_bias, gitem_gamma\n",
    "        lambda_b = lambda_user_bias\n",
    "    else:\n",
    "        dict1, bias1, gamma1 = usersPerItem, item_bias, item_gamma\n",
    "        dict2, bias2, gamma2 = itemsPerUser, user_bias, user_gamma\n",
    "        gbias1, ggamma1 = gitem_bias, gitem_gamma\n",
    "        gbias2, ggamma2 = guser_bias, guser_gamma\n",
    "        lambda_b = lambda_item_bias\n",
    "    #\n",
    "    for id1 in dict1.keys():\n",
    "        n_rows = len(dict1[id1])\n",
    "        ratings = np.zeros(n_rows)\n",
    "        alphas = np.full(n_rows, alpha)\n",
    "        biases, gammas = np.zeros((n_rows, (1 + len(g_keys))*2)), np.zeros((n_rows, (1 + len(g_keys))*2, k))\n",
    "        G, G2, H = np.zeros((n_rows,k)), np.zeros((n_rows,k)), np.zeros((n_rows,k))\n",
    "        # gamma_term = np.zeros(n_rows)\n",
    "        ### Get id1 terms\n",
    "        biases1 = get_biases(id1, [bias1] + get_gterms(gbias1, g_keys))\n",
    "        gammas1 = get_gammas(id1, k, [gamma1] + get_gterms(ggamma1, g_keys))\n",
    "        #\n",
    "        for i,(id2, rating) in enumerate(dict1[id1]):\n",
    "            ### Get id2 terms\n",
    "            biases2 = get_biases(id2, [bias2] + get_gterms(gbias2, g_keys))\n",
    "            gammas2 = get_gammas(id2, k, [gamma2] + get_gterms(ggamma2, g_keys))\n",
    "            ### Extract values\n",
    "            ratings[i] = rating\n",
    "            biases[i,:] = np.array(biases1 + biases2)\n",
    "            gammas_i = np.array(gammas1 + gammas2)\n",
    "            gammas[i,:,:] = gammas_i\n",
    "            G[i,:] = np.sum(gammas_i, axis=0)\n",
    "            G2[i,:] = np.sum(gammas_i, axis=0)**2\n",
    "            H[i,:]  = np.sum(gammas_i**2, axis=0)\n",
    "        gamma_term = (1/2) * (G2 - H).sum(axis=1)\n",
    "        ### Update each bias\n",
    "        for exclude_ind,exclude_bias in enumerate([None] + g_keys):\n",
    "            if exclude_bias: \n",
    "                if id1 not in gbias1[exclude_bias]: continue\n",
    "                bias_val = gbias1[exclude_bias][id1][1]\n",
    "            else: \n",
    "                bias_val = bias1[id1][1]\n",
    "            biases[:,exclude_ind] = np.zeros(n_rows)\n",
    "            bias_term = biases.sum(axis=1)\n",
    "            bias_numer = bias_val * (ratings - alphas - bias_term - gamma_term).sum()\n",
    "            bias_denom = (n_rows * bias_val**2) + lambda_b\n",
    "            new_bias = bias_numer / bias_denom\n",
    "            if exclude_bias: \n",
    "                gbias1[exclude_bias][id1] = (new_bias, bias_val)\n",
    "                biases[:,exclude_ind] = np.full(n_rows, new_bias * bias_val)\n",
    "            else:\n",
    "                bias1[id1] = (new_bias, bias_val)\n",
    "                biases[:,exclude_ind] = np.full(n_rows, new_bias * bias_val)\n",
    "\n",
    "    if update_type == \"user\":\n",
    "        return (alpha, bias1, bias2, gamma1, gamma2, gbias1, gbias2, ggamma1, ggamma2, g_keys)\n",
    "    else:\n",
    "        return (alpha, bias2, bias1, gamma2, gamma1, gbias2, gbias1, ggamma2, ggamma1, g_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc3ca29-d67b-4d8e-968d-7bcaa32545ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Coordinate Descent for FM --> GAMMA\n",
    "# def update_gamma(theta, lambdas, k, update_type, exclude_key=None):\n",
    "#     \"\"\"\n",
    "#     Update bias1 to dbias1\n",
    "#     \"\"\"\n",
    "#     alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "#     if update_type == \"user\":\n",
    "#         dict1, bias1, gamma1 = itemsPerUser, user_bias, user_gamma\n",
    "#         dict2, bias2, gamma2 = usersPerItem, item_bias, item_gamma\n",
    "#         gbias1, ggamma1 = guser_bias, guser_gamma\n",
    "#         gbias2, ggamma2 = gitem_bias, gitem_gamma\n",
    "#         lambda_b, lambda_g = lambdas[\"lambda_user_bias\"], lambdas[\"lambda_user_gamma\"]\n",
    "#     else:\n",
    "#         dict1, bias1, gamma1 = usersPerItem, item_bias, item_gamma\n",
    "#         dict2, bias2, gamma2 = itemsPerUser, user_bias, user_gamma\n",
    "#         gbias1, ggamma1 = gitem_bias, gitem_gamma\n",
    "#         gbias2, ggamma2 = guser_bias, guser_gamma\n",
    "#         lambda_b, lambda_g = lambdas[\"lambda_item_bias\"], lambdas[\"lambda_item_gamma\"]\n",
    "#     #\n",
    "#     if exclude_key:\n",
    "#         gamma = ggamma1[exclude_key].copy()\n",
    "#         exclude_ind = g_keys.index(exclude_key)\n",
    "#         print(exclude_ind)\n",
    "#     else:\n",
    "#         gamma = gamma1.copy()\n",
    "#     # print(exclude_key, exclude_ind)\n",
    "#     for id1 in gamma.keys():\n",
    "#         n_rows = len(dict1[id1])\n",
    "#         gamma_val = gamma[id1][1]\n",
    "#         #\n",
    "#         numer_term = 0\n",
    "#         denom_term = 0\n",
    "#         # Get id1 terms\n",
    "#         bias1_0, gamma1_0 = get_bias(id1, bias1), get_gamma(id1, k, gamma1)\n",
    "#         gbiases1, ggammas1 = get_other_bias_gamma(id1, k, g_keys, gbias1, ggamma1)\n",
    "#         if exclude_key: \n",
    "#             ggammas1[g_keys.index(exclude_key)] = np.zeros(k)\n",
    "#         else: \n",
    "#             gamma1_0  = np.zeros(k)\n",
    "#         gammas1 = [gamma1_0] + ggammas1\n",
    "#         for i,(id2,rating) in enumerate(dict1[id1]):\n",
    "#             # Get id2 terms\n",
    "#             bias2_0, gamma2_0 = get_bias(id2, bias2), get_gamma(id2, k, gamma2)\n",
    "#             gbiases2, ggammas2 = get_other_bias_gamma(id2, k, g_keys, gbias2, ggamma2)\n",
    "#             # Get summations\n",
    "#             biases = np.array([bias1_0, bias2_0] + gbiases1 + gbiases2)\n",
    "#             gammas = np.array(gammas1 + [gamma2_0] + ggammas2)\n",
    "#             A = rating - alpha - biases.sum()\n",
    "#             C = gammas.sum(axis=0)\n",
    "#             C2 = np.square(gammas.sum(axis=0))\n",
    "#             D = np.sum(np.square(gammas), axis=0)\n",
    "#             numer_term += C * (A - ((1/2) * (C2 - D)))\n",
    "#             denom_term += C2\n",
    "#         gamma[id1] = ((gamma_val * numer_term) / ((denom_term * gamma_val**2) + lambda_g), gamma_val)\n",
    "#     if exclude_key:\n",
    "#         ggamma1[exclude_key] = gamma.copy()\n",
    "#     else:\n",
    "#         gamma1 = gamma.copy()\n",
    "\n",
    "#     if update_type == \"user\":\n",
    "#         return (alpha, bias1, bias2, gamma1, gamma2, gbias1, gbias2, ggamma1, ggamma2, g_keys)\n",
    "#     else:\n",
    "#         return (alpha, bias2, bias1, gamma2, gamma1, gbias2, gbias1, ggamma2, ggamma1, g_keys)\n",
    "\n",
    "# def update_gamma(theta, lambdas, k, update_type, exclude_key=None):\n",
    "#     \"\"\"\n",
    "#     Update bias1 to dbias1\n",
    "#     \"\"\"\n",
    "#     alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "#     if update_type == \"user\":\n",
    "#         dict1, bias1, gamma1 = itemsPerUser, user_bias, user_gamma\n",
    "#         dict2, bias2, gamma2 = usersPerItem, item_bias, item_gamma\n",
    "#         gbias1, ggamma1 = guser_bias, guser_gamma\n",
    "#         gbias2, ggamma2 = gitem_bias, gitem_gamma\n",
    "#         lambda_b, lambda_g = lambdas[\"lambda_user_bias\"], lambdas[\"lambda_user_gamma\"]\n",
    "#     else:\n",
    "#         dict1, bias1, gamma1 = usersPerItem, item_bias, item_gamma\n",
    "#         dict2, bias2, gamma2 = itemsPerUser, user_bias, user_gamma\n",
    "#         gbias1, ggamma1 = gitem_bias, gitem_gamma\n",
    "#         gbias2, ggamma2 = guser_bias, guser_gamma\n",
    "#         lambda_b, lambda_g = lambdas[\"lambda_item_bias\"], lambdas[\"lambda_item_gamma\"]\n",
    "#     #\n",
    "#     if exclude_key:\n",
    "#         gamma = ggamma1[exclude_key].copy()\n",
    "#         exclude_ind = g_keys.index(exclude_key)\n",
    "#         print(exclude_ind)\n",
    "#     else:\n",
    "#         gamma = gamma1.copy()\n",
    "#     # print(exclude_key, exclude_ind)\n",
    "#     for id1 in gamma.keys():\n",
    "#         n_rows = len(dict1[id1])\n",
    "#         gamma_val = gamma[id1][1]\n",
    "#         #\n",
    "#         numer_term = 0\n",
    "#         denom_term = 0\n",
    "#         # Get id1 terms\n",
    "#         bias1_0, gamma1_0 = get_bias(id1, bias1), get_gamma(id1, k, gamma1)\n",
    "#         gbiases1, ggammas1 = get_other_bias_gamma(id1, k, g_keys, gbias1, ggamma1)\n",
    "#         if exclude_key: \n",
    "#             ggammas1[g_keys.index(exclude_key)] = np.zeros(k)\n",
    "#         else: \n",
    "#             gamma1_0  = np.zeros(k)\n",
    "#         gammas1 = [gamma1_0] + ggammas1\n",
    "#         for i,(id2,rating) in enumerate(dict1[id1]):\n",
    "#             # Get id2 terms\n",
    "#             bias2_0, gamma2_0 = get_bias(id2, bias2), get_gamma(id2, k, gamma2)\n",
    "#             gbiases2, ggammas2 = get_other_bias_gamma(id2, k, g_keys, gbias2, ggamma2)\n",
    "#             # Get summations\n",
    "#             biases = np.array([bias1_0, bias2_0] + gbiases1 + gbiases2)\n",
    "#             gammas = np.array(gammas1 + [gamma2_0] + ggammas2)\n",
    "#             A = rating - alpha - biases.sum()\n",
    "#             C = gammas.sum(axis=0)\n",
    "#             C2 = np.square(gammas.sum(axis=0))\n",
    "#             D = np.sum(np.square(gammas), axis=0)\n",
    "#             numer_term += C * (A - ((1/2) * (C2 - D)))\n",
    "#             denom_term += C2\n",
    "#         gamma[id1] = ((gamma_val * numer_term) / ((denom_term * gamma_val**2) + lambda_g), gamma_val)\n",
    "#     if exclude_key:\n",
    "#         ggamma1[exclude_key] = gamma.copy()\n",
    "#     else:\n",
    "#         gamma1 = gamma.copy()\n",
    "\n",
    "#     if update_type == \"user\":\n",
    "#         return (alpha, bias1, bias2, gamma1, gamma2, gbias1, gbias2, ggamma1, ggamma2, g_keys)\n",
    "#     else:\n",
    "#         return (alpha, bias2, bias1, gamma2, gamma1, gbias2, gbias1, ggamma2, ggamma1, g_keys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5829014d-3126-470c-a112-6ea1ae3d211f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ### Coordinate descent update (TERMS AND ALG, individual gammas)\n",
    "\n",
    "# ### Coordinate Descent for FM --> TERMS\n",
    "# def update_terms(theta, lambdas, k, update_type, exclude_key=None):\n",
    "#     \"\"\"\n",
    "#     Update bias1 to dbias1\n",
    "#     \"\"\"\n",
    "#     alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "#     lambda_user_bias, lambda_user_gamma = lambdas[\"lambda_user_bias\"], lambdas[\"lambda_user_gamma\"]\n",
    "#     lambda_item_bias, lambda_item_gamma = lambdas[\"lambda_item_bias\"], lambdas[\"lambda_item_gamma\"]\n",
    "#     if update_type == \"user\":\n",
    "#         dict1, bias1, gamma1 = itemsPerUser, user_bias, user_gamma\n",
    "#         dict2, bias2, gamma2 = usersPerItem, item_bias, item_gamma\n",
    "#         gbias1, ggamma1 = guser_bias, guser_gamma\n",
    "#         gbias2, ggamma2 = gitem_bias, gitem_gamma\n",
    "#         lambda_b, lambda_g = lambda_user_bias, np.full(k, lambda_user_gamma)\n",
    "#     else:\n",
    "#         dict1, bias1, gamma1 = usersPerItem, item_bias, item_gamma\n",
    "#         dict2, bias2, gamma2 = itemsPerUser, user_bias, user_gamma\n",
    "#         gbias1, ggamma1 = gitem_bias, gitem_gamma\n",
    "#         gbias2, ggamma2 = guser_bias, guser_gamma\n",
    "#         lambda_b, lambda_g = lambda_item_bias, np.full(k, lambda_item_gamma)\n",
    "#     # Define the excluded gamma index\n",
    "#     if exclude_key: exclude_ind = g_keys.index(exclude_key) + 1\n",
    "#     else: exclude_ind = 0\n",
    "\n",
    "#     for id1 in dict1.keys():\n",
    "#         n_rows = len(dict1[id1])\n",
    "#         ratings = np.zeros(n_rows)\n",
    "#         alphas = np.full(n_rows, alpha)\n",
    "#         biases, gammas = np.zeros((n_rows, (1 + len(g_keys))*2)), np.zeros((n_rows, (1 + len(g_keys))*2, k))\n",
    "#         C, C2, D = np.zeros((n_rows,k)), np.zeros((n_rows,k)), np.zeros((n_rows,k))\n",
    "#         # gamma_term = np.zeros(n_rows)\n",
    "#         ### Get id1 terms\n",
    "#         biases1 = get_biases(id1, [bias1] + get_gterms(gbias1, g_keys))\n",
    "#         gammas1 = get_gammas(id1, k, [gamma1] + get_gterms(ggamma1, g_keys))\n",
    "#         #\n",
    "#         for i,(id2, rating) in enumerate(dict1[id1]):\n",
    "#             ### Get id2 terms\n",
    "#             biases2 = get_biases(id2, [bias2] + get_gterms(gbias2, g_keys))\n",
    "#             gammas2 = get_gammas(id2, k, [gamma2] + get_gterms(ggamma2, g_keys))\n",
    "#             ### Extract values\n",
    "#             ratings[i] = rating\n",
    "#             biases[i,:] = np.array(biases1 + biases2)\n",
    "#             gammas_i = np.array(gammas1 + gammas2)\n",
    "#             gammas[i,:,:] = gammas_i\n",
    "#             C[i,:] = np.sum(gammas_i, axis=0)\n",
    "#             C2[i,:] = np.sum(gammas_i, axis=0)**2\n",
    "#             D[i,:]  = np.sum(gammas_i**2, axis=0)\n",
    "#         gamma_term = (1/2) * (C2 - D).sum(axis=1)\n",
    "#         ### Update each bias\n",
    "#         for exclude_ind,exclude_bias in enumerate([None] + g_keys):\n",
    "#             if exclude_bias: \n",
    "#                 if id1 not in gbias1[exclude_bias]: continue\n",
    "#                 bias_val = gbias1[exclude_bias][id1][1]\n",
    "#             else: \n",
    "#                 bias_val = bias1[id1][1]\n",
    "#             biases[:,exclude_ind] = np.zeros(n_rows)\n",
    "#             bias_term = biases.sum(axis=1)\n",
    "#             bias_numer = bias_val * (ratings - alphas - bias_term - gamma_term).sum()\n",
    "#             bias_denom = (n_rows * bias_val**2) + lambda_b\n",
    "#             new_bias = bias_numer / bias_denom\n",
    "#             if exclude_bias: \n",
    "#                 gbias1[exclude_bias][id1] = (new_bias, bias_val)\n",
    "#                 biases[:,exclude_ind] = np.full(n_rows, new_bias * bias_val)\n",
    "#             else:\n",
    "#                 bias1[id1] = (new_bias, bias_val)\n",
    "#                 biases[:,exclude_ind] = np.full(n_rows, new_bias * bias_val)\n",
    "#         ### GAMMA ACTIONS\n",
    "#         rating_term = np.repeat(ratings[:, np.newaxis], k, axis=1)\n",
    "#         alpha_term = np.repeat(alphas[:, np.newaxis], k, axis=1)\n",
    "#         bias_term = np.repeat(biases.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "#         ### Update the specified gamma term if id1 is in gamma\n",
    "#         if exclude_key:\n",
    "#             if id1 not in ggamma1[exclude_key]: continue\n",
    "#             else:\n",
    "#                 gamma_val = ggamma1[exclude_key][id1][1]\n",
    "#                 exclude_ind = g_keys.index(exclude_key) + 1\n",
    "#         else: \n",
    "#             gamma_val = gamma1[id1][1]\n",
    "#             exclude_ind = 0\n",
    "#         gammas[:,exclude_ind,:] = np.zeros((n_rows, k))\n",
    "#         C  = np.sum(gammas, axis=1)\n",
    "#         C2 = C**2\n",
    "#         D  = np.sum(gammas**2, axis=1)\n",
    "#         #\n",
    "#         C_forGamma = np.repeat(C.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "#         C2_forGamma = np.repeat(C2.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "#         D_forGamma = np.repeat(D.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "#         gamma_term2 = (1/2) * (D_forGamma - C2_forGamma)\n",
    "#         # print(C.shape, rating_term.shape, alpha_term.shape, bias_term.shape, gamma_term2.shape, C2.shape)\n",
    "#         #\n",
    "#         gamma_numer = np.sum(C_forGamma * (gamma_term2 + rating_term - alpha_term - bias_term), axis=0) * gamma_val\n",
    "#         gamma_denom = np.sum(np.repeat(C2_forGamma.sum(axis=1)[:, np.newaxis], k, axis=1), axis=0) * gamma_val**2\n",
    "#         # print(gamma_numer.shape, gamma_denom.shape)\n",
    "#         if exclude_key:\n",
    "#             ggamma1[exclude_key][id1] = (np.array(gamma_numer / (gamma_denom + lambda_g)), gamma_val)\n",
    "#         else:\n",
    "#             gamma1[id1] = (np.array(gamma_numer / (gamma_denom + lambda_g)), gamma_val)\n",
    "\n",
    "#     if update_type == \"user\":\n",
    "#         return (alpha, bias1, bias2, gamma1, gamma2, gbias1, gbias2, ggamma1, ggamma2, g_keys)\n",
    "#     else:\n",
    "#         return (alpha, bias2, bias1, gamma2, gamma1, gbias2, gbias1, ggamma2, ggamma1, g_keys)\n",
    "\n",
    "\n",
    "# def update_params_coordinateDescent(theta, lambdas, Xtrain, ytrain, k, quiet2=True):\n",
    "#     \"\"\"\n",
    "#     Update parameters based on how well they predict in their CURRENT states\n",
    "#     Coordinate descent instead of gradient descent for faster convergence\n",
    "#     ###\n",
    "#     Latent factor model\n",
    "#          Fix user_gamma --> iterate and update alpha, user_bias, item_gamma\n",
    "#          Fix item_gamma --> iterate and update alpha, user_bias, user_gamma\n",
    "#     Fix user_gamma... Fix item_gamma...\n",
    "#     Repeat the above until model have converged\n",
    "#     \"\"\"\n",
    "#     alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "#     # Get cycle count\n",
    "#     exclude_keys = [None, None]\n",
    "#     for key in g_keys:\n",
    "#         if len(guser_gamma[key]) > 0:\n",
    "#             exclude_keys.append(key)\n",
    "#         if len(gitem_gamma[key]) > 0:\n",
    "#             exclude_keys.append(key)\n",
    "#     n_cycles = len(exclude_keys)\n",
    "#     # Get iteration params\n",
    "#     exclude_keys = [None, None]\n",
    "#     n_guser_gammas = len(guser_gamma)\n",
    "#     for key in g_keys:\n",
    "#         exclude_keys.extend([key, key])\n",
    "#     # n_cycles = len(exclude_keys)\n",
    "\n",
    "#     iter_counter = 0\n",
    "#     for cycle,exclude_key in enumerate(exclude_keys):\n",
    "#         if (cycle > 1):\n",
    "#             if cycle % 2:\n",
    "#                 # Check guser gamma\n",
    "#                 if len(guser_gamma[exclude_key]) == 0: continue\n",
    "#             else:\n",
    "#                 # Check gitem gamma\n",
    "#                 if len(gitem_gamma[exclude_key]) == 0: continue\n",
    "#         iter_counter += 1\n",
    "#         if not quiet2:\n",
    "#             print(f\"Starting Cycle {iter_counter}/{n_cycles} --> Exclude key {exclude_key}\")\n",
    "#         # Set iteration parameters\n",
    "        \n",
    "#         t_cycle = time.time()\n",
    "#         ### ALPHA\n",
    "#         t0 = time.time()\n",
    "#         theta = update_alpha(theta, lambdas, Xtrain, ytrain, k)\n",
    "#         if not quiet2:\n",
    "#             print(f\"   Alpha Complete --> Time elapsed: {time.time() - t0}\")\n",
    "#             # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k)) ###\n",
    "#         ### Cycle between user/item updates\n",
    "#         if not cycle % 2:\n",
    "#             # Item biases\n",
    "#             t0 = time.time()\n",
    "#             theta = update_biases(theta, lambdas, k, \"item\")\n",
    "#             if not quiet2:\n",
    "#                 print(f\"   Item Biases complete --> Time elapsed: {time.time() - t0}\")\n",
    "#                 # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k)) ###\n",
    "#             # User biases and gamma\n",
    "#             t0 = time.time()\n",
    "#             theta = update_terms(theta, lambdas, k, \"user\", exclude_key)\n",
    "#             if not quiet2:\n",
    "#                 print(f\"   User Biases and Gamma ({exclude_key}) Complete --> Time elapsed: {time.time() - t0}\")\n",
    "#                 # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k)) ###\n",
    "#         else:\n",
    "#             ### User biases\n",
    "#             t0 = time.time()\n",
    "#             theta = update_biases(theta, lambdas, k, \"user\")\n",
    "#             if not quiet2:\n",
    "#                 print(f\"   User Biases complete --> Time elapsed: {time.time() - t0}\")\n",
    "#                 # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k)) ###\n",
    "#             # Item biases and gamma\n",
    "#             t0 = time.time()\n",
    "#             theta = update_terms(theta, lambdas, k, \"item\", exclude_key)\n",
    "#             if not quiet2:\n",
    "#                 print(f\"   Item Biases and Gamma ({exclude_key}) Complete --> Time elapsed: {time.time() - t0}\")\n",
    "#                 # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k)) ###\n",
    "\n",
    "#         if not quiet2:\n",
    "#             print(f\"Cycle {iter_counter}/{n_cycles} Complete --> Time elapsed: {round(time.time() - t_cycle, 3)}s\")\n",
    "#         print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k)) ###\n",
    "\n",
    "#     return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f180a167-0fb2-4394-98b4-9833b54aaf23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Coordinate descent update (TERMS AND ALG, gamma simultaneous)\n",
    "\n",
    "### Coordinate Descent for FM --> TERMS\n",
    "def update_terms(theta, lambdas, k, update_type):\n",
    "    \"\"\"\n",
    "    Update bias1 to dbias1\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "    lambda_user_bias, lambda_user_gamma = lambdas[\"lambda_user_bias\"], lambdas[\"lambda_user_gamma\"]\n",
    "    lambda_item_bias, lambda_item_gamma = lambdas[\"lambda_item_bias\"], lambdas[\"lambda_item_gamma\"]\n",
    "    if update_type == \"user\":\n",
    "        dict1, bias1, gamma1 = itemsPerUser, user_bias, user_gamma\n",
    "        dict2, bias2, gamma2 = usersPerItem, item_bias, item_gamma\n",
    "        gbias1, ggamma1 = guser_bias, guser_gamma\n",
    "        gbias2, ggamma2 = gitem_bias, gitem_gamma\n",
    "        lambda_b, lambda_g = lambda_user_bias, lambda_user_gamma\n",
    "    else:\n",
    "        dict1, bias1, gamma1 = usersPerItem, item_bias, item_gamma\n",
    "        dict2, bias2, gamma2 = itemsPerUser, user_bias, user_gamma\n",
    "        gbias1, ggamma1 = gitem_bias, gitem_gamma\n",
    "        gbias2, ggamma2 = guser_bias, guser_gamma\n",
    "        lambda_b, lambda_g = lambda_item_bias, lambda_item_gamma\n",
    "        \n",
    "    # Define the excluded gamma index\n",
    "    for id1 in dict1.keys():\n",
    "        n_rows = len(dict1[id1])\n",
    "        ratings = np.zeros(n_rows)\n",
    "        alphas = np.full(n_rows, alpha)\n",
    "        biases, gammas = np.zeros((n_rows, (1 + len(g_keys))*2)), np.zeros((n_rows, (1 + len(g_keys))*2, k))\n",
    "        C, C2, D = np.zeros((n_rows,k)), np.zeros((n_rows,k)), np.zeros((n_rows,k))\n",
    "        # gamma_term = np.zeros(n_rows)\n",
    "        ### Get id1 terms\n",
    "        biases1 = get_biases(id1, [bias1] + get_gterms(gbias1, g_keys))\n",
    "        gammas1 = get_gammas(id1, k, [gamma1] + get_gterms(ggamma1, g_keys))\n",
    "        #\n",
    "        for i,(id2, rating) in enumerate(dict1[id1]):\n",
    "            ### Get id2 terms\n",
    "            biases2 = get_biases(id2, [bias2] + get_gterms(gbias2, g_keys))\n",
    "            gammas2 = get_gammas(id2, k, [gamma2] + get_gterms(ggamma2, g_keys))\n",
    "            ### Extract values\n",
    "            ratings[i] = rating\n",
    "            biases[i,:] = np.array(biases1 + biases2)\n",
    "            gammas_i = np.array(gammas1 + gammas2)\n",
    "            gammas[i,:,:] = gammas_i\n",
    "            C[i,:] = np.sum(gammas_i, axis=0)\n",
    "            C2[i,:] = np.sum(gammas_i, axis=0)**2\n",
    "            D[i,:]  = np.sum(gammas_i**2, axis=0)\n",
    "        gamma_term = (1/2) * (C2 - D).sum(axis=1)\n",
    "        ### Update each bias\n",
    "        for exclude_ind,exclude_bias in enumerate([None] + g_keys):\n",
    "            if exclude_bias: \n",
    "                if id1 not in gbias1[exclude_bias]: continue\n",
    "                bias_val = gbias1[exclude_bias][id1][1]\n",
    "            else: \n",
    "                bias_val = bias1[id1][1]\n",
    "            biases[:,exclude_ind] = np.zeros(n_rows)\n",
    "            bias_term = biases.sum(axis=1)\n",
    "            bias_numer = bias_val * (ratings - alphas - bias_term - gamma_term).sum()\n",
    "            bias_denom = (n_rows * bias_val**2) + lambda_b\n",
    "            new_bias = bias_numer / bias_denom\n",
    "            if exclude_bias: \n",
    "                gbias1[exclude_bias][id1] = (new_bias, bias_val)\n",
    "                biases[:,exclude_ind] = np.full(n_rows, new_bias * bias_val)\n",
    "            else:\n",
    "                bias1[id1] = (new_bias, bias_val)\n",
    "                biases[:,exclude_ind] = np.full(n_rows, new_bias * bias_val)\n",
    "        ### GAMMA ACTIONS\n",
    "        for exclude_ind,exclude_key in enumerate([None] + g_keys):\n",
    "            if exclude_key: \n",
    "                if id1 not in ggamma1[exclude_key]: continue\n",
    "                gamma_val = ggamma1[exclude_key][id1][1]\n",
    "            else: \n",
    "                gamma_val = gamma1[id1][1]\n",
    "            rating_term = np.repeat(ratings[:, np.newaxis], k, axis=1)\n",
    "            alpha_term = np.repeat(alphas[:, np.newaxis], k, axis=1)\n",
    "            bias_term = np.repeat(biases.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "            #\n",
    "            gammas[:,exclude_ind,:] = np.zeros((n_rows, k))\n",
    "            C  = np.sum(gammas, axis=1)\n",
    "            C2 = C**2\n",
    "            D  = np.sum(gammas**2, axis=1)\n",
    "            #\n",
    "            C_forGamma = np.repeat(C.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "            C2_forGamma = np.repeat(C2.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "            D_forGamma = np.repeat(D.sum(axis=1)[:, np.newaxis], k, axis=1)\n",
    "            gamma_term2 = (1/2) * (D_forGamma - C2_forGamma)\n",
    "            # print(C.shape, rating_term.shape, alpha_term.shape, bias_term.shape, gamma_term2.shape, C2.shape)\n",
    "            #\n",
    "            gamma_numer = np.sum(C_forGamma * (gamma_term2 + rating_term - alpha_term - bias_term), axis=0) * gamma_val\n",
    "            gamma_denom = np.sum(np.repeat(C2_forGamma.sum(axis=1)[:, np.newaxis], k, axis=1), axis=0) * gamma_val**2\n",
    "            new_gamma = np.array(gamma_numer / (gamma_denom + lambda_g))\n",
    "            # print(gamma_numer.shape, gamma_denom.shape)\n",
    "            if exclude_key:\n",
    "                ggamma1[exclude_key][id1] = (new_gamma, gamma_val)\n",
    "            else:\n",
    "                gamma1[id1] = (new_gamma, gamma_val)\n",
    "            gammas[:,exclude_ind,:] = np.repeat((new_gamma * gamma_val)[np.newaxis, :], n_rows, axis=0) \n",
    "\n",
    "    if update_type == \"user\":\n",
    "        return (alpha, bias1, bias2, gamma1, gamma2, gbias1, gbias2, ggamma1, ggamma2, g_keys)\n",
    "    else:\n",
    "        return (alpha, bias2, bias1, gamma2, gamma1, gbias2, gbias1, ggamma2, ggamma1, g_keys)\n",
    "\n",
    "def update_params_coordinateDescent(theta, lambdas, Xtrain, ytrain, k, quiet2=True):\n",
    "    \"\"\"\n",
    "    Update parameters based on how well they predict in their CURRENT states\n",
    "    Coordinate descent instead of gradient descent for faster convergence\n",
    "    ###\n",
    "    Latent factor model\n",
    "         Fix user_gamma --> iterate and update alpha, user_bias, item_gamma\n",
    "         Fix item_gamma --> iterate and update alpha, user_bias, user_gamma\n",
    "    Fix user_gamma... Fix item_gamma...\n",
    "    Repeat the above until model have converged\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "    # Get cycle count\n",
    "    n_cycles = 2\n",
    "\n",
    "    for cycle in range(n_cycles):\n",
    "        if not quiet2:\n",
    "            print(f\"Starting Cycle {cycle + 1}/{n_cycles}\")\n",
    "        t_cycle = time.time()\n",
    "        \n",
    "        ### ALPHA\n",
    "        t0 = time.time()\n",
    "        theta = update_alpha(theta, lambdas, Xtrain, ytrain, k)\n",
    "        # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "        if not quiet2:\n",
    "            print(f\"   Alpha Complete --> Time elapsed: {time.time() - t0}\")\n",
    "            \n",
    "        ### Cycle between user/item updates\n",
    "        if cycle % 2:\n",
    "            # Item biases\n",
    "            t0 = time.time()\n",
    "            theta = update_biases(theta, lambdas, k, \"item\")\n",
    "            # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "            if not quiet2:\n",
    "                print(f\"   Item Biases complete --> Time elapsed: {time.time() - t0}\")\n",
    "            # User biases and gammas\n",
    "            t0 = time.time()\n",
    "            theta = update_terms(theta, lambdas, k, \"user\")\n",
    "            # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "            if not quiet2:\n",
    "                print(f\"   User Biases and Gammas Complete --> Time elapsed: {time.time() - t0}\")\n",
    "        else:\n",
    "            ### User biases\n",
    "            t0 = time.time()\n",
    "            theta = update_biases(theta, lambdas, k, \"user\")\n",
    "            # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "            if not quiet2:\n",
    "                print(f\"   User Biases complete --> Time elapsed: {time.time() - t0}\")\n",
    "            # Item biases and gammas\n",
    "            t0 = time.time()\n",
    "            theta = update_terms(theta, lambdas, k, \"item\")\n",
    "            # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "            if not quiet2:\n",
    "                print(f\"   Item Biases and Gammas Complete --> Time elapsed: {time.time() - t0}\")\n",
    "\n",
    "        # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "        if not quiet2:\n",
    "            print(f\"Cycle {cycle + 1}/{n_cycles} Complete --> Time elapsed: {round(time.time() - t_cycle, 3)}s\")\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014f33ea-93e3-418c-8e91-7fe28222a9ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Coordinate descent update (Fastest)\n",
    "def update_params_coordinateDescent(theta, lambdas, Xtrain, ytrain, k, quiet2=True):\n",
    "    \"\"\"\n",
    "    Update parameters based on how well they predict in their CURRENT states\n",
    "    Coordinate descent instead of gradient descent for faster convergence\n",
    "    ###\n",
    "    Latent factor model\n",
    "         Fix user_gamma --> iterate and update alpha, user_bias, item_gamma\n",
    "         Fix item_gamma --> iterate and update alpha, user_bias, user_gamma\n",
    "    Fix user_gamma... Fix item_gamma...\n",
    "    Repeat the above until model have converged\n",
    "    \"\"\"\n",
    "    alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = theta\n",
    "\n",
    "    ### ALPHA\n",
    "    t0 = time.time()\n",
    "    theta = update_alpha(theta, lambdas, Xtrain, ytrain, k)\n",
    "    # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "    if not quiet2:\n",
    "        print(f\"   Alpha Complete --> Time elapsed: {time.time() - t0}\")\n",
    "    ### Item biases and gamma\n",
    "    t0 = time.time()\n",
    "    theta = update_terms(theta, lambdas, k, \"item\")\n",
    "    # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "    if not quiet2:\n",
    "        print(f\"   Item Biases and Gammas complete --> Time elapsed: {time.time() - t0}\")\n",
    "    ### User biases and gammas\n",
    "    t0 = time.time()\n",
    "    theta = update_terms(theta, lambdas, k, \"user\")\n",
    "    # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "    if not quiet2:\n",
    "        print(f\"   User Biases and Gammas Complete --> Time elapsed: {time.time() - t0}\")\n",
    "\n",
    "    # print(\"   \", get_cost_mse(theta, lambdas, Xtrain, ytrain, k))\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71797786-1619-4ee2-9a2d-e01266368bf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### OTHER HELPFUL FUNCTIONS\n",
    "######################################\n",
    "def get_rec_structs(train_data):\n",
    "    \"\"\"\n",
    "    Extract stats used for creating the classifier features\n",
    "    Input is (user_id, item_id, value), ...\n",
    "    Typically value is rating, but can be other things (e.g., hours played)\n",
    "\n",
    "    itemsPerUser: Records each item in the training set that each user interacted with (along with the corresponding value)\n",
    "    usersPerItem: Records each user in the training set that each item interacted with (along with the corresponding value)\n",
    "    valueDict: Records the value for each (user, item) tuple\n",
    "    userAverages: Gives the average value for each user\n",
    "    itemAverages: Gives the average value for each item\n",
    "\n",
    "    \"\"\"\n",
    "    ### Record which items each user interacted with and which users interacted with which item\n",
    "    itemsPerUser = defaultdict(list)\n",
    "    usersPerItem = defaultdict(list)\n",
    "    userItemDict, itemUserDict = {}, {}\n",
    "    for u,b,v in train_data:\n",
    "        itemsPerUser[u].append((b,v))\n",
    "        usersPerItem[b].append((u,v))\n",
    "        userItemDict[(u,b)] = v\n",
    "        itemUserDict[(b,u)] = v\n",
    "\n",
    "    ### Calculate user and item average ratings\n",
    "    userAverages = {}\n",
    "    itemAverages = {}\n",
    "    for u,tuples in itemsPerUser.items():\n",
    "        values = [value for item,value in tuples]\n",
    "        # values = [value for item,value in tuples if value != 0]\n",
    "        # if len(values) == 0: continue\n",
    "        userAverages[u] = sum(values) / len(values)\n",
    "    for i,tuples in usersPerItem.items():\n",
    "        values = [value for user,value in tuples]\n",
    "        # values = [value for user,value in tuples if value != 0]\n",
    "        # if len(values) == 0: continue\n",
    "        itemAverages[i] = sum(values) / len(values)\n",
    "\n",
    "    rec_structs = {\"itemsPerUser\":itemsPerUser,\n",
    "                  \"usersPerItem\":usersPerItem,\n",
    "                  \"userItemDict\":userItemDict,\n",
    "                  \"itemUserDict\":itemUserDict,\n",
    "                  \"userAverages\":userAverages,\n",
    "                  \"itemAverages\":itemAverages,}\n",
    "\n",
    "    return rec_structs\n",
    "\n",
    "def unpack_rec_structs(rec_structs):\n",
    "    \"\"\"\n",
    "    Take the input recommender_structs and return the itemized contents\n",
    "    \"\"\"\n",
    "    itemsPerUser = rec_structs[\"itemsPerUser\"]\n",
    "    usersPerItem = rec_structs[\"usersPerItem\"]\n",
    "    userItemDict = rec_structs[\"userItemDict\"]\n",
    "    itemUserDict = rec_structs[\"itemUserDict\"]\n",
    "    userAverages = rec_structs[\"userAverages\"]\n",
    "    itemAverages = rec_structs[\"itemAverages\"]\n",
    "\n",
    "    return (itemsPerUser, usersPerItem, userItemDict, itemUserDict, userAverages, itemAverages)\n",
    "\n",
    "def pd_get_rec_structs(df, user_col, item_col, val_col, user_limit=None, item_limit=None, only_nonzero=None, nested_dicts=False):\n",
    "    \"\"\"\n",
    "    Like get_rec_structs(), but is customized for the pandas library\n",
    "    Extract stats used for creating the classifier features\n",
    "    Input is df which has at least a user_col, item_col, val_col, ...\n",
    "    Typically value is rating, but can be other things (e.g., hours played)\n",
    "\n",
    "    itemsPerUser: Records each item in the training set that each user interacted with (along with the corresponding value)\n",
    "    usersPerItem: Records each user in the training set that each item interacted with (along with the corresponding value)\n",
    "    valueDict: Records the value for each (user, item) tuple\n",
    "    userAverages: Gives the average value for each user\n",
    "    itemAverages: Gives the average value for each item\n",
    "\n",
    "    user/item limit allows for estimating with less data to save time\n",
    "    only_nonzero: Only track users/items that have non-zero values\n",
    "    nested_dicts: for itemsPerUser and usersPerItem, output a nested dict instead of a list of tuples\n",
    "    \"\"\"\n",
    "    if only_nonzero is None: only_nonzero = False\n",
    "\n",
    "    dfu, dfi = df.copy(), df.copy()\n",
    "    dfu[\"itemsPerUser\"] = list(zip(dfu[item_col], dfu[val_col]))\n",
    "    dfi[\"usersPerItem\"] = list(zip(dfi[user_col], dfi[val_col]))\n",
    "    dfu[\"valueDict_userItem\"] = list(zip(dfu[user_col], dfu[item_col]))\n",
    "    dfi[\"valueDict_itemUser\"] = list(zip(dfi[item_col], dfi[user_col]))\n",
    "    #\n",
    "    if user_limit is None: user_limit = len(dfu)\n",
    "    if item_limit is None: item_limit = len(dfi)\n",
    "    # Limit data to only_nonzero if necessary\n",
    "    if only_nonzero:\n",
    "        dfu = dfu[dfu[val_col] > 0]\n",
    "        dfi = dfi[dfi[val_col] > 0]\n",
    "    if nested_dicts:\n",
    "        itemsPerUser = dfu.groupby(user_col)[\"itemsPerUser\"].apply(lambda x: list(x)[:item_limit]).apply(lambda x: {key:val for key,val in x}).to_dict()\n",
    "        usersPerItem = dfi.groupby(item_col)[\"usersPerItem\"].apply(lambda x: list(x)[:user_limit]).apply(lambda x: {key:val for key,val in x}).to_dict()\n",
    "    else:\n",
    "        itemsPerUser = dfu.groupby(user_col)[\"itemsPerUser\"].apply(lambda x: list(x)[:item_limit]).to_dict()\n",
    "        usersPerItem = dfi.groupby(item_col)[\"usersPerItem\"].apply(lambda x: list(x)[:user_limit]).to_dict()\n",
    "\n",
    "    valueDict = dfu[[\"valueDict_userItem\", val_col]].drop_duplicates().set_index(\"valueDict_userItem\", drop=True).to_dict()[val_col]\n",
    "    meanValue = np.mean([val for val in valueDict.values()])\n",
    "    medianValue = np.median([val for val in valueDict.values()])\n",
    "    userAverages = dfu.groupby(user_col)[val_col].mean().to_dict()\n",
    "    itemAverages = dfi.groupby(item_col)[val_col].mean().to_dict()\n",
    "\n",
    "    user_structs = [itemsPerUser, userAverages]\n",
    "    item_structs = [usersPerItem, itemAverages]\n",
    "\n",
    "    return user_structs, item_structs, valueDict, meanValue, medianValue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2623e7-cc95-4f9e-bd26-070220cf64a9",
   "metadata": {},
   "source": [
    "### Change number of genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf62fa57-9488-4be7-a7eb-db17f829258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Params\n",
    "\n",
    "# Category params\n",
    "offset = 0\n",
    "n_categories = 21\n",
    "include_other = True # True if other categories are condensed into \"Other\" --> False if ONLY the desired categories are included\n",
    "user_weight_cutoff = 0 # 0 if all played games contribute to the weight --> 3.8 if only games where playtime is above Q1 is included (for example)\n",
    "\n",
    "# weight_df_type --> \"genres\" for genres, \"tags\" for tags\n",
    "weight_df_type = \"tags\"\n",
    "# Other genre --> weight type = 1 (indiciates presence of other genre) or \"count\" (weight based on number of other genres)\n",
    "weight_type = \"count\"\n",
    "\n",
    "### Model params\n",
    "k = 5\n",
    "lambda_bias = 1\n",
    "lambda_gamma = 5\n",
    "ep = 0.0005\n",
    "iter_limit = 300\n",
    "output_bounds = (0, np.inf)\n",
    "init_bounds1 = (-.1, .1) # For regular params\n",
    "init_bounds2 = (-.5, .5) # For genre params\n",
    "\n",
    "quiet = False\n",
    "quiet2 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8723e9b-e2aa-4422-bdcb-8f548af6dbc0",
   "metadata": {},
   "source": [
    "### FACTORIZATION MACHINE BASED ON GENRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ba5df7-5fe0-4fe2-81de-1bf1e292f1b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Load review data and set up additional weight dataframes\n",
    "seed = 100\n",
    "\n",
    "# Train/valid/train2/test\n",
    "train_filepath = \"data/train_df2.csv\"\n",
    "valid_filepath = \"data/valid_df.csv\"\n",
    "test_filepath = \"data/test_df.csv\"\n",
    "\n",
    "# Item data\n",
    "item_filepath1 = \"data/game_genres_df.csv\"\n",
    "item_filepath2 = \"data/game_tags_df.csv\"\n",
    "\n",
    "# Set up columns\n",
    "user_col, item_col = \"user_id\", \"item_id\"\n",
    "class_col = \"playtime_log\"\n",
    "binary_class_col = \"playtime_binary\"\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(train_filepath)\n",
    "valid_df = pd.read_csv(valid_filepath)\n",
    "test_df = pd.read_csv(test_filepath)\n",
    "game_genres_df = pd.read_csv(item_filepath1, index_col=item_col)\n",
    "game_tags_df = pd.read_csv(item_filepath2, index_col=item_col)\n",
    "\n",
    "### Set up additional weight dataframes\n",
    "\n",
    "if n_categories==0:\n",
    "    item_weight_df = pd.DataFrame()\n",
    "    user_weight_df = pd.DataFrame()\n",
    "else:\n",
    "    if weight_df_type == \"genres\":\n",
    "        item_weight_df = game_genres_df.loc[:, game_genres_df.sum(axis=0).sort_values(ascending=False).index]\n",
    "    elif weight_df_type == \"tags\":\n",
    "        item_weight_df = game_tags_df.loc[:, game_tags_df.sum(axis=0).sort_values(ascending=False).index]\n",
    "    else:\n",
    "        print(\"Invalid category column\")\n",
    "\n",
    "    ### Set up item category columns\n",
    "    all_category_cols = item_weight_df.columns.to_list()\n",
    "    include_cols = all_category_cols[offset:offset + n_categories]\n",
    "    exclude_cols = list(set(all_category_cols).difference(set(include_cols)))\n",
    "    other_col = \"Other\"\n",
    "    include_other_cols = include_cols + [other_col]\n",
    "    \n",
    "    ### Set up item_weight_df\n",
    "    # Limit to only items from train_df\n",
    "    item_weight_df = item_weight_df[item_weight_df.index.isin(train_df[item_col].unique())]\n",
    "    # For each item, weights are determined by which genres are applied to each item\n",
    "    # weights for each item should add up to one\n",
    "    other_item_weight_df = item_weight_df[exclude_cols].sum(axis=1).rename(other_col)\n",
    "    other_item_weight_df\n",
    "    item_weight_df = item_weight_df[include_cols]\n",
    "    if include_other:\n",
    "        item_weight_df = pd.merge(item_weight_df, other_item_weight_df, on=item_col, how=\"left\")\n",
    "    item_weight_df = item_weight_df.div(item_weight_df.sum(axis=1), axis=0).fillna(0).reset_index(drop=False)\n",
    "\n",
    "    ### Set up user_weight_df\n",
    "    # For each user, weights are determined by # hours played divided by # games each user played (per category)\n",
    "    # Again, all weights should add up to one\n",
    "    user_weight_df2 = train_df[[user_col, item_col, class_col, binary_class_col]]\n",
    "    user_weight_df2 = pd.merge(user_weight_df2, item_weight_df, on=item_col, how=\"left\").fillna(0)\n",
    "    if include_other:\n",
    "        agg_cols = include_other_cols\n",
    "    else:\n",
    "        agg_cols = include_cols\n",
    "    play_agg = user_weight_df2[user_weight_df2[class_col] > user_weight_cutoff].groupby(user_col).sum()[agg_cols]\n",
    "    count_agg = user_weight_df2[user_weight_df2[class_col] > user_weight_cutoff].groupby(user_col).count()[agg_cols]\n",
    "    user_weight_df = play_agg * count_agg\n",
    "    user_weight_df = user_weight_df.div(user_weight_df.sum(axis=1), axis=0).fillna(0).reset_index(drop=False)\n",
    "\n",
    "if n_categories==0:\n",
    "    include_cols = []\n",
    "else:\n",
    "    if include_other:\n",
    "        include_cols = include_other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a6cf6e-3e52-416b-923e-976daf698e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>Indie</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Singleplayer</th>\n",
       "      <th>Casual</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>RPG</th>\n",
       "      <th>Simulation</th>\n",
       "      <th>Multiplayer</th>\n",
       "      <th>...</th>\n",
       "      <th>Atmospheric</th>\n",
       "      <th>Sci-fi</th>\n",
       "      <th>Platformer</th>\n",
       "      <th>Co-op</th>\n",
       "      <th>Open World</th>\n",
       "      <th>Shooter</th>\n",
       "      <th>Story Rich</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>282010.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1640.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1630.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3800.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    item_id  Indie    Action  Adventure  Singleplayer  Casual  Strategy  \\\n",
       "0  282010.0  0.125  0.125000       0.00      0.125000     0.0  0.000000   \n",
       "1      70.0  0.000  0.050000       0.05      0.050000     0.0  0.000000   \n",
       "2    1640.0  0.000  0.000000       0.00      0.000000     0.0  0.076923   \n",
       "3    1630.0  0.000  0.000000       0.00      0.000000     0.0  0.125000   \n",
       "4    2400.0  0.050  0.050000       0.05      0.050000     0.0  0.050000   \n",
       "5    3800.0  0.000  0.166667       0.00      0.166667     0.0  0.000000   \n",
       "\n",
       "        RPG  Simulation  Multiplayer  ...  Atmospheric    Sci-fi  Platformer  \\\n",
       "0  0.000000        0.00     0.125000  ...     0.000000  0.000000         0.0   \n",
       "1  0.000000        0.00     0.050000  ...     0.050000  0.050000         0.0   \n",
       "2  0.076923        0.00     0.076923  ...     0.076923  0.000000         0.0   \n",
       "3  0.125000        0.00     0.000000  ...     0.125000  0.000000         0.0   \n",
       "4  0.050000        0.05     0.050000  ...     0.000000  0.000000         0.0   \n",
       "5  0.000000        0.00     0.000000  ...     0.000000  0.166667         0.0   \n",
       "\n",
       "   Co-op  Open World  Shooter  Story Rich   Fantasy  Horror     Other  \n",
       "0    0.0         0.0     0.00    0.000000  0.000000     0.0  0.500000  \n",
       "1    0.0         0.0     0.05    0.050000  0.000000     0.0  0.550000  \n",
       "2    0.0         0.0     0.00    0.000000  0.076923     0.0  0.538462  \n",
       "3    0.0         0.0     0.00    0.000000  0.125000     0.0  0.500000  \n",
       "4    0.0         0.0     0.05    0.000000  0.000000     0.0  0.550000  \n",
       "5    0.0         0.0     0.00    0.166667  0.000000     0.0  0.166667  \n",
       "\n",
       "[6 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_weight_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd3777d5-5bed-47bc-b716-290566de2ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>Indie</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Singleplayer</th>\n",
       "      <th>Casual</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>RPG</th>\n",
       "      <th>Simulation</th>\n",
       "      <th>Multiplayer</th>\n",
       "      <th>...</th>\n",
       "      <th>Atmospheric</th>\n",
       "      <th>Sci-fi</th>\n",
       "      <th>Platformer</th>\n",
       "      <th>Co-op</th>\n",
       "      <th>Open World</th>\n",
       "      <th>Shooter</th>\n",
       "      <th>Story Rich</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u0</td>\n",
       "      <td>0.028645</td>\n",
       "      <td>0.044587</td>\n",
       "      <td>0.041304</td>\n",
       "      <td>0.046760</td>\n",
       "      <td>0.011253</td>\n",
       "      <td>0.017775</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.022123</td>\n",
       "      <td>0.033717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019565</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.019565</td>\n",
       "      <td>0.015942</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.015217</td>\n",
       "      <td>0.580648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u1</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.047222</td>\n",
       "      <td>0.030556</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u10</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.034266</td>\n",
       "      <td>0.043357</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.034266</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.025175</td>\n",
       "      <td>0.038811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.029720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031818</td>\n",
       "      <td>0.029720</td>\n",
       "      <td>0.031818</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u100</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.046429</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u1000</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.050155</td>\n",
       "      <td>0.039316</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.016239</td>\n",
       "      <td>0.038617</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.033916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026923</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.026923</td>\n",
       "      <td>0.030070</td>\n",
       "      <td>0.026923</td>\n",
       "      <td>0.035470</td>\n",
       "      <td>0.023232</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.564452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>u10000</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.457407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id     Indie    Action  Adventure  Singleplayer    Casual  Strategy  \\\n",
       "0      u0  0.028645  0.044587   0.041304      0.046760  0.011253  0.017775   \n",
       "1      u1  0.016667  0.055556   0.047222      0.030556  0.025000  0.016667   \n",
       "2     u10  0.013636  0.036364   0.034266      0.043357  0.013636  0.034266   \n",
       "3    u100  0.053571  0.042857   0.035714      0.039286  0.000000  0.032143   \n",
       "4   u1000  0.003846  0.045455   0.050155      0.039316  0.003846  0.016239   \n",
       "5  u10000  0.059259  0.064815   0.037037      0.059259  0.022222  0.037037   \n",
       "\n",
       "        RPG  Simulation  Multiplayer  ...  Atmospheric    Sci-fi  Platformer  \\\n",
       "0  0.010870    0.022123     0.033717  ...     0.019565  0.006522    0.006522   \n",
       "1  0.038889    0.016667     0.055556  ...     0.000000  0.008333    0.008333   \n",
       "2  0.018182    0.025175     0.038811  ...     0.022727  0.029720    0.000000   \n",
       "3  0.028571    0.007143     0.042857  ...     0.035714  0.046429    0.014286   \n",
       "4  0.038617    0.003846     0.033916  ...     0.026923  0.011538    0.003846   \n",
       "5  0.033333    0.022222     0.042593  ...     0.016667  0.011111    0.005556   \n",
       "\n",
       "      Co-op  Open World   Shooter  Story Rich   Fantasy    Horror     Other  \n",
       "0  0.021739    0.010870  0.019565    0.015942  0.006522  0.015217  0.580648  \n",
       "1  0.041667    0.038889  0.025000    0.000000  0.022222  0.000000  0.519444  \n",
       "2  0.031818    0.029720  0.031818    0.013636  0.004545  0.000000  0.560140  \n",
       "3  0.028571    0.035714  0.021429    0.000000  0.000000  0.014286  0.485714  \n",
       "4  0.026923    0.030070  0.026923    0.035470  0.023232  0.007692  0.564452  \n",
       "5  0.027778    0.016667  0.011111    0.011111  0.005556  0.005556  0.457407  \n",
       "\n",
       "[6 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_weight_df.head(6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dc0f3ef-c01d-4471-85ce-66c269fe3184",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows in training set: 676910\n",
      "# rows in test set: 169227\n",
      "\n",
      "k = 5\n",
      "# Params for Indie --> Users: 13196 Items: 4958\n",
      "# Params for Action --> Users: 14238 Items: 3952\n",
      "# Params for Adventure --> Users: 13936 Items: 3391\n",
      "# Params for Singleplayer --> Users: 13995 Items: 2923\n",
      "# Params for Casual --> Users: 11920 Items: 2452\n",
      "# Params for Strategy --> Users: 13229 Items: 2013\n",
      "# Params for RPG --> Users: 12550 Items: 1617\n",
      "# Params for Simulation --> Users: 12612 Items: 1509\n",
      "# Params for Multiplayer --> Users: 14194 Items: 1546\n",
      "# Params for Great Soundtrack --> Users: 12082 Items: 1300\n",
      "# Params for Puzzle --> Users: 9115 Items: 1131\n",
      "# Params for 2D --> Users: 10519 Items: 1121\n",
      "# Params for Atmospheric --> Users: 13000 Items: 1125\n",
      "# Params for Sci-fi --> Users: 11792 Items: 885\n",
      "# Params for Platformer --> Users: 9557 Items: 835\n",
      "# Params for Co-op --> Users: 14026 Items: 841\n",
      "# Params for Open World --> Users: 13141 Items: 827\n",
      "# Params for Shooter --> Users: 13811 Items: 826\n",
      "# Params for Story Rich --> Users: 10145 Items: 796\n",
      "# Params for Fantasy --> Users: 9336 Items: 757\n",
      "# Params for Horror --> Users: 11502 Items: 750\n",
      "# Params for Other --> Users: 14275 Items: 7040\n",
      "\n",
      "# of regular user weights: 14551\n",
      "# of regular item weights: 9214\n",
      "\n",
      "g_keys: ['Indie', 'Action', 'Adventure', 'Singleplayer', 'Casual', 'Strategy', 'RPG', 'Simulation', 'Multiplayer', 'Great Soundtrack', 'Puzzle', '2D', 'Atmospheric', 'Sci-fi', 'Platformer', 'Co-op', 'Open World', 'Shooter', 'Story Rich', 'Fantasy', 'Horror', 'Other']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Further split the data into X/y pairs for convenience\n",
    "X_cols = [user_col, item_col]# + genre_cols\n",
    "X_train, y_train = train_df[X_cols], train_df[class_col]\n",
    "# X_valid, y_valid = valid_df[X_cols], valid_df[class_col]\n",
    "X_test, y_test = test_df[X_cols], test_df[class_col]\n",
    "\n",
    "print(f\"# rows in training set: {len(X_train)}\")\n",
    "print(f\"# rows in test set: {len(X_test)}\")\n",
    "print()\n",
    "\n",
    "##### GET GENRE REC STRUCTS\n",
    "category_dicts = {}\n",
    "weight_cols = include_cols\n",
    "for weight_col in weight_cols:\n",
    "    category_dicts[weight_col] = (get_id_weights(user_weight_df, user_col, weight_col, only_nonzero=True), \n",
    "                               get_id_weights(item_weight_df, item_col, weight_col, only_nonzero=True))\n",
    "    # category_dicts[weight_col] = (get_id_weights(user_weight_df, user_col, weight_col, only_nonzero=True), \n",
    "    #                            {})\n",
    "    # category_dicts[weight_col] = (get_id_weights(train_df, user_col, weight_col, only_nonzero=True), \n",
    "    #                            get_id_weights(train_df, item_col, weight_col, only_nonzero=True))\n",
    "    # category_dicts[weight_col] = ({}, \n",
    "    #                            get_id_weights(train_df, item_col, weight_col, only_nonzero=True))\n",
    "\n",
    "print(f\"k = {k}\")\n",
    "# print(f\"Included {weight_df_type}: {include_cols}\")\n",
    "for category in include_cols:\n",
    "    print(f\"# Params for {category} --> Users: {len(category_dicts[category][0])} Items: {len(category_dicts[category][1])}\")\n",
    "print()\n",
    "\n",
    "##### USERS/ITEMS\n",
    "weight_col = binary_class_col\n",
    "rec_structs = pd_get_rec_structs(train_df, user_col, item_col, class_col, only_nonzero=False)\n",
    "(itemsPerUser, userAverages), (usersPerItem, itemAverages), valueDict, meanValue, medianValue = rec_structs\n",
    "userWeights = get_id_weights(train_df, user_col, weight_type=1)\n",
    "itemWeights = get_id_weights(train_df, item_col, weight_type=1)\n",
    "\n",
    "print(f\"# of regular user weights: {len(userWeights)}\")\n",
    "print(f\"# of regular item weights: {len(itemWeights)}\")\n",
    "print()\n",
    "\n",
    "##### Run the code\n",
    "### Initialize parameters\n",
    "lambda_user_bias = lambda_bias\n",
    "lambda_user_gamma = lambda_gamma\n",
    "lambda_item_bias = lambda_bias\n",
    "lambda_item_gamma = lambda_gamma\n",
    "lambdas = {\"lambda_user_bias\":lambda_user_bias, \"lambda_user_gamma\":lambda_user_gamma,\n",
    "          \"lambda_item_bias\":lambda_item_bias, \"lambda_item_gamma\":lambda_item_gamma}\n",
    "\n",
    "params = {\n",
    "         \"lambdas\":lambdas, \"k\": k, \"ep\": ep, \"iter_limit\": iter_limit, \"output_bounds\": output_bounds\n",
    "         }\n",
    "\n",
    "# Get theta\n",
    "alpha = meanValue\n",
    "user_bias, item_bias, user_gamma, item_gamma = {}, {}, {}, {}\n",
    "user_bias, item_bias, user_gamma, item_gamma = initialize_weighted_params(userWeights, itemWeights, k, init_bounds1)\n",
    "guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys = {}, {}, {}, {}, []\n",
    "for category in include_cols:\n",
    "    ubias, ibias, ugamma, igamma = initialize_weighted_params(category_dicts[category][0], category_dicts[category][1], k, init_bounds2)\n",
    "    guser_bias[category] = ubias\n",
    "    gitem_bias[category] = ibias\n",
    "    guser_gamma[category] = ugamma\n",
    "    gitem_gamma[category] = igamma\n",
    "    g_keys.append(category)\n",
    "theta = alpha, user_bias, item_bias, user_gamma, item_gamma, guser_bias, gitem_bias, guser_gamma, gitem_gamma, g_keys\n",
    "print(f\"g_keys: {g_keys}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ecfeaf2-49ec-41c9-9d5e-0096ba8ecb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-03 02:16:19.471445\n",
      "Fitting parameters...\n",
      "-----\n",
      "-----> Epoch 1: Cost = 3092170.163531657, Train MSE = 4.436093898834942, Time Elapsed = 193.72585487365723\n",
      "-----> Epoch 2: Cost = 2980946.1868234114, Train MSE = 4.21012081775692, Time Elapsed = 183.6024534702301\n",
      "-----> Epoch 3: Cost = 2938226.405641048, Train MSE = 4.135051584968841, Time Elapsed = 168.87027597427368\n",
      "-----> Epoch 4: Cost = 2914789.685964188, Train MSE = 4.09831519089181, Time Elapsed = 170.07898569107056\n",
      "-----> Epoch 5: Cost = 2899996.647315487, Train MSE = 4.077388032427215, Time Elapsed = 168.8663091659546\n",
      "-----> Epoch 6: Cost = 2889593.2900178987, Train MSE = 4.064203709117427, Time Elapsed = 170.2659933567047\n",
      "-----> Epoch 7: Cost = 2881838.6600503284, Train MSE = 4.0550792981586365, Time Elapsed = 170.35956382751465\n",
      "-----> Epoch 8: Cost = 2875534.820130243, Train MSE = 4.048390600881008, Time Elapsed = 170.59467720985413\n",
      "-----> Epoch 9: Cost = 2870176.6508963173, Train MSE = 4.043080943836841, Time Elapsed = 170.49643898010254\n",
      "-----> Epoch 10: Cost = 2865563.5579971005, Train MSE = 4.038574042823186, Time Elapsed = 170.09818196296692\n",
      "-----> Epoch 11: Cost = 2861530.5159267327, Train MSE = 4.034585095783494, Time Elapsed = 170.57265877723694\n",
      "-----> Epoch 12: Cost = 2857869.2821834823, Train MSE = 4.030976744935285, Time Elapsed = 172.5423936843872\n",
      "-----> Epoch 13: Cost = 2854546.9208051474, Train MSE = 4.027612290726962, Time Elapsed = 171.71735739707947\n",
      "-----> Epoch 14: Cost = 2851418.5542280995, Train MSE = 4.024505859046553, Time Elapsed = 170.72348141670227\n",
      "-----> Epoch 15: Cost = 2848521.3438644814, Train MSE = 4.021617041253623, Time Elapsed = 171.50475931167603\n",
      "-----> Epoch 16: Cost = 2845792.971833482, Train MSE = 4.018829413003386, Time Elapsed = 171.85136604309082\n",
      "-----> Epoch 17: Cost = 2843134.3104641745, Train MSE = 4.016033344312373, Time Elapsed = 172.30047130584717\n",
      "-----> Epoch 18: Cost = 2840590.6695991894, Train MSE = 4.013228964294719, Time Elapsed = 172.4903645515442\n",
      "-----> Epoch 19: Cost = 2838239.2889520214, Train MSE = 4.010893181402506, Time Elapsed = 172.40584087371826\n",
      "-----> Epoch 20: Cost = 2836247.52042844, Train MSE = 4.009017053518797, Time Elapsed = 172.3660180568695\n",
      "-----> Epoch 21: Cost = 2834592.175041247, Train MSE = 4.007617333978512, Time Elapsed = 171.8295876979828\n",
      "-----> Epoch 22: Cost = 2833195.5258121598, Train MSE = 4.006624833705847, Time Elapsed = 170.65509343147278\n",
      "-----> Epoch 23: Cost = 2831999.520495372, Train MSE = 4.005859137659564, Time Elapsed = 172.38950777053833\n",
      "-----> Epoch 24: Cost = 2830993.376145885, Train MSE = 4.005261156617255, Time Elapsed = 171.9372842311859\n",
      "-----> Epoch 25: Cost = 2830140.2964792885, Train MSE = 4.004902162188081, Time Elapsed = 172.140141248703\n",
      "Convergence after 25 epochs: Cost = 2830140.2964792885, Train MSE = 4.004902162188081, Total Time Elapsed = 4314.386059999466\n",
      "Prediction time: 10.181345701217651\n",
      "\n",
      "Test MSE = 4.255251569932521\n"
     ]
    }
   ],
   "source": [
    "### Fit the model to the training data\n",
    "print(datetime.now())\n",
    "theta, cost, mse = fit_parameters(X_train.values.tolist(), y_train, theta, quiet=quiet, quiet2=quiet2, **params)\n",
    "\n",
    "### Predict using the learned parameters on the test set\n",
    "t0 = time.time()\n",
    "predictions = predict_latent_factor_batch(X_test.values.tolist(), theta, k, output_bounds)\n",
    "print(f\"Prediction time: {time.time() - t0}\")\n",
    "testMSE = get_MSE(predictions, y_test)\n",
    "\n",
    "print()\n",
    "print(f\"Test MSE = {testMSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67ed4d0-7001-4519-b6d0-3dbf498181bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 10.191340208053589\n",
      "\n",
      "Test MSE = 4.255251569932521\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "predictions = predict_latent_factor_batch(X_test.values.tolist(), theta, k, output_bounds)\n",
    "print(f\"Prediction time: {time.time() - t0}\")\n",
    "testMSE = get_MSE(predictions, y_test)\n",
    "\n",
    "print()\n",
    "print(f\"Test MSE = {testMSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "924c4c12-878d-4325-9de0-2b61cbf6e16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>playtime_log</th>\n",
       "      <th>pred_21tags_withOther__k5_lb1_lg5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u5084</td>\n",
       "      <td>31130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.873749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u11861</td>\n",
       "      <td>233270</td>\n",
       "      <td>6.573680</td>\n",
       "      <td>3.437156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u4949</td>\n",
       "      <td>220200</td>\n",
       "      <td>5.451038</td>\n",
       "      <td>6.513286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u1808</td>\n",
       "      <td>4000</td>\n",
       "      <td>5.446737</td>\n",
       "      <td>7.794845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u5401</td>\n",
       "      <td>1280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.397015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169222</th>\n",
       "      <td>u11442</td>\n",
       "      <td>238960</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>3.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169223</th>\n",
       "      <td>u12710</td>\n",
       "      <td>4000</td>\n",
       "      <td>9.467847</td>\n",
       "      <td>8.135887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169224</th>\n",
       "      <td>u10496</td>\n",
       "      <td>8190</td>\n",
       "      <td>5.337538</td>\n",
       "      <td>4.922942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169225</th>\n",
       "      <td>u6112</td>\n",
       "      <td>48700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.590327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169226</th>\n",
       "      <td>u1492</td>\n",
       "      <td>39000</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>4.303831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169227 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  playtime_log  pred_21tags_withOther__k5_lb1_lg5\n",
       "0        u5084    31130      0.000000                           0.873749\n",
       "1       u11861   233270      6.573680                           3.437156\n",
       "2        u4949   220200      5.451038                           6.513286\n",
       "3        u1808     4000      5.446737                           7.794845\n",
       "4        u5401     1280      0.000000                           1.397015\n",
       "...        ...      ...           ...                                ...\n",
       "169222  u11442   238960      3.044522                           3.077500\n",
       "169223  u12710     4000      9.467847                           8.135887\n",
       "169224  u10496     8190      5.337538                           4.922942\n",
       "169225   u6112    48700      0.000000                           6.590327\n",
       "169226   u1492    39000      4.110874                           4.303831\n",
       "\n",
       "[169227 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save results\n",
    "output = test_df[[\"user_id\", \"item_id\", \"playtime_log\"]].copy()\n",
    "if (len(user_weight_df)==0) and (len(item_weight_df)==0):\n",
    "    output[f\"pred_FM_NoOtherParams__k{k}_lb{lambda_bias}_lg{lambda_gamma}\"] = predictions\n",
    "    output.to_csv(f\"data/test_results_FM_NoOtherParams_k{k}_lb{lambda_bias}_lg{lambda_gamma}.csv\", index=False)\n",
    "else:\n",
    "    if include_other:\n",
    "        output[f\"pred_{n_categories}{weight_df_type}_withOther__k{k}_lb{lambda_bias}_lg{lambda_gamma}\"] = predictions\n",
    "        output.to_csv(f\"data/test_results_FM_{n_categories}{weight_df_type}_withOther_k{k}_lb{lambda_bias}_lg{lambda_gamma}.csv\", index=False)\n",
    "    else:\n",
    "        output[f\"pred_{n_categories}{weight_df_type}__k{k}_lb{lambda_bias}_lg{lambda_gamma}\"] = predictions\n",
    "        output.to_csv(f\"data/test_results_FM_{n_categories}{weight_df_type}_k{k}_lb{lambda_bias}_lg{lambda_gamma}.csv\", index=False)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
